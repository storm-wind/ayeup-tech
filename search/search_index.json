{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Synaptic Recall Some notes on how I got around some issues with my work within IT. Most of the notes on here I have curated from different forums and blogs etc. These solutions worked for my own situation so they may not work for yours. I found this to be the case with a lot fo solutions out there for different problems.","title":"Home"},{"location":"#synaptic-recall","text":"Some notes on how I got around some issues with my work within IT. Most of the notes on here I have curated from different forums and blogs etc. These solutions worked for my own situation so they may not work for yours. I found this to be the case with a lot fo solutions out there for different problems.","title":"Synaptic Recall"},{"location":"databases/mysql-ops/","text":"Typical Operations of MySQL/MariaDB How to purge binary logs mysql -u root -p SHOW BINARY LOGS; and then purge from selectd binary files. Command below will purge logs from last week. PURGE BINARY LOGS BEFORE DATE(NOW() - INTERVAL 7 DAY) + INTERVAL 0 SECOND; You can set binary logs to expire as well. In the /etc/my.cnf . expire_logs_days = 7; To set it without restart MYSQL you can run this, SET GLOBAL expire_logs_days = 7; Also setting max binlog file size as above. max_binlog_size = 100M SET GLOBAL max_binlog_size = 100M; MySQL Replication Show status of secondary replication server Log into MySQL shell and run this, SHOW SLAVE STATUS \\G Then you'll see something like this, MariaDB [(none)]> SHOW SLAVE STATUS \\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: hostname.example.com Master_User: replication_user Master_Port: 3306 Connect_Retry: 10 Master_Log_File: primary1-bin.000198 Read_Master_Log_Pos: 204348276 Relay_Log_File: hostname-relay-bin.000043 Relay_Log_Pos: 204348574 Relay_Master_Log_File: primary1-bin.000198 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 204348276 Relay_Log_Space: 204348929 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_SSL_Crl: Master_SSL_Crlpath: Using_Gtid: No Gtid_IO_Pos: Replicate_Do_Domain_Ids: Replicate_Ignore_Domain_Ids: Parallel_Mode: conservative SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it 1 row in set (0.00 sec) Turn off GTID I had to do this as the version I was replicating from was 5.5 to 10.2. 5.5 doesn;t support GTID judging by the errors so I tunred it off. mysql> stop slave; Query OK, 0 rows affected (0.00 sec) mysql> change master to master_auto_position=0; Query OK, 0 rows affected (0.00 sec) mysql> change master to master_host='master-1.db.example.com' , master_port = 3306 , master_user = 'replicate' , master_password='welcome' , master_log_file = 'bin_log.000003' , master_log_pos = 191, master_connect_retry = 5; Query OK, 0 rows affected, 2 warnings (0.00 sec) mysql> show warnings; +-------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Level | Code | Message | +-------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Note | 1759 | Sending passwords in plain text without SSL/TLS is extremely insecure. | | Note | 1760 | Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the 'START SLAVE Syntax' in the MySQL Manual for more information. | +-------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 2 rows in set (0.00 sec) mysql> start slave; Query OK, 0 rows affected (0.00 sec) mysql> SHOW SLAVE STATUS \\G This will show GTID status still but we can make sure its not using it by running this, SHOW PROCESSLIST \\G Look for Command: Binlog Dump which was Command: Binlog Dump GTID","title":"MySQL Ops"},{"location":"databases/mysql-ops/#typical-operations-of-mysqlmariadb","text":"","title":"Typical Operations of MySQL/MariaDB"},{"location":"databases/mysql-ops/#how-to-purge-binary-logs","text":"mysql -u root -p SHOW BINARY LOGS; and then purge from selectd binary files. Command below will purge logs from last week. PURGE BINARY LOGS BEFORE DATE(NOW() - INTERVAL 7 DAY) + INTERVAL 0 SECOND; You can set binary logs to expire as well. In the /etc/my.cnf . expire_logs_days = 7; To set it without restart MYSQL you can run this, SET GLOBAL expire_logs_days = 7; Also setting max binlog file size as above. max_binlog_size = 100M SET GLOBAL max_binlog_size = 100M;","title":"How to purge binary logs"},{"location":"databases/mysql-ops/#mysql-replication","text":"","title":"MySQL Replication"},{"location":"databases/mysql-ops/#show-status-of-secondary-replication-server","text":"Log into MySQL shell and run this, SHOW SLAVE STATUS \\G Then you'll see something like this, MariaDB [(none)]> SHOW SLAVE STATUS \\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: hostname.example.com Master_User: replication_user Master_Port: 3306 Connect_Retry: 10 Master_Log_File: primary1-bin.000198 Read_Master_Log_Pos: 204348276 Relay_Log_File: hostname-relay-bin.000043 Relay_Log_Pos: 204348574 Relay_Master_Log_File: primary1-bin.000198 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 204348276 Relay_Log_Space: 204348929 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_SSL_Crl: Master_SSL_Crlpath: Using_Gtid: No Gtid_IO_Pos: Replicate_Do_Domain_Ids: Replicate_Ignore_Domain_Ids: Parallel_Mode: conservative SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it 1 row in set (0.00 sec)","title":"Show status of secondary replication server"},{"location":"databases/mysql-ops/#turn-off-gtid","text":"I had to do this as the version I was replicating from was 5.5 to 10.2. 5.5 doesn;t support GTID judging by the errors so I tunred it off. mysql> stop slave; Query OK, 0 rows affected (0.00 sec) mysql> change master to master_auto_position=0; Query OK, 0 rows affected (0.00 sec) mysql> change master to master_host='master-1.db.example.com' , master_port = 3306 , master_user = 'replicate' , master_password='welcome' , master_log_file = 'bin_log.000003' , master_log_pos = 191, master_connect_retry = 5; Query OK, 0 rows affected, 2 warnings (0.00 sec) mysql> show warnings; +-------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Level | Code | Message | +-------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Note | 1759 | Sending passwords in plain text without SSL/TLS is extremely insecure. | | Note | 1760 | Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the 'START SLAVE Syntax' in the MySQL Manual for more information. | +-------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 2 rows in set (0.00 sec) mysql> start slave; Query OK, 0 rows affected (0.00 sec) mysql> SHOW SLAVE STATUS \\G This will show GTID status still but we can make sure its not using it by running this, SHOW PROCESSLIST \\G Look for Command: Binlog Dump which was Command: Binlog Dump GTID","title":"Turn off GTID"},{"location":"databases/mysql-rep/","text":"MySQL Replication and associated Commands How to setup MySQL/MariaDB replication between two hosts Working with Binary Log files Binary log files keep track of any changes to the database, no changes then no logs. Binary log files are setup as part of replication - see above. How to Expire Binary logs after a period of time When I setup replication the binary log files quickly filled up the filesystem and with me not being a DBA didn't realise. So I had to setup log files to expire. The default is 0 whcih means they never expire. Heres how to do it mysql -u <username> -p SHOW VARIABLES LIKE 'expire_logs_days'; -> ; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | expire_logs_days | 0 | +------------------+-------+ 1 row in set (0.00 sec) SET GLOBAL expire_logs_days=<num of days>; Show Replication Status SHOW SLAVE STATUS\\G And you'll get something like this. *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: source1 Master_User: root Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000004 Read_Master_Log_Pos: 931 Relay_Log_File: slave1-relay-bin.000056 Relay_Log_Pos: 950 Relay_Master_Log_File: mysql-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 931 Relay_Log_Space: 1365 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: 0","title":"Replication"},{"location":"databases/mysql-rep/#mysql-replication-and-associated-commands","text":"","title":"MySQL Replication and associated Commands"},{"location":"databases/mysql-rep/#how-to-setup-mysqlmariadb-replication-between-two-hosts","text":"","title":"How to setup MySQL/MariaDB replication between two hosts"},{"location":"databases/mysql-rep/#working-with-binary-log-files","text":"Binary log files keep track of any changes to the database, no changes then no logs. Binary log files are setup as part of replication - see above.","title":"Working with Binary Log files"},{"location":"databases/mysql-rep/#how-to-expire-binary-logs-after-a-period-of-time","text":"When I setup replication the binary log files quickly filled up the filesystem and with me not being a DBA didn't realise. So I had to setup log files to expire. The default is 0 whcih means they never expire. Heres how to do it mysql -u <username> -p SHOW VARIABLES LIKE 'expire_logs_days'; -> ; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | expire_logs_days | 0 | +------------------+-------+ 1 row in set (0.00 sec) SET GLOBAL expire_logs_days=<num of days>;","title":"How to Expire Binary logs after a period of time"},{"location":"databases/mysql-rep/#show-replication-status","text":"SHOW SLAVE STATUS\\G And you'll get something like this. *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: source1 Master_User: root Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000004 Read_Master_Log_Pos: 931 Relay_Log_File: slave1-relay-bin.000056 Relay_Log_Pos: 950 Relay_Master_Log_File: mysql-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 931 Relay_Log_Space: 1365 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: 0","title":"Show Replication Status"},{"location":"databases/mysql-rep/#_1","text":"","title":""},{"location":"databases/mysqlgen/","text":"General MYSQL/MARIADB Commands Quickly check integrity of MySQL DB mysqlcheck --all-databases --quick --fast Override MySql file limits systemctl edit mysqld.service then add this [service] LimitNOFILE=10000 Change parameters in MYSQL Navigate to /etc/my.cnf Increase MySQL/MariaDB max_connections online (without a restart) You can use SHOW VARIABLE LIKE 'max_connections'; to check the number of max connections allowed. mysql> SHOW VARIABLE LIKE 'max_connections'; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 151 | +-----------------+-------+ 1 row in set (0.01 sec) You might want to see how many connections you have in your database. You can use the below query to check that. mysql> SELECT COUNT(1) FROM information_schema.processlist; +----------+ | COUNT(1) | +----------+ | 100 | +----------+ 1 row in set (0.00 sec) Let\u2019s assume, we need to increase maximum connections allowed to 250. Please run below query. mysql> SET GLOBAL max_connections = 250; Query OK, 0 rows affected (0.00 sec) You can verify the change by running SHOW VARIABLE LIKE 'max_connections'; . mysql> SHOW VARIABLE LIKE 'max_connections'; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 250 | +-----------------+-------+ 1 row in set (0.01 sec) But our work is not done yet. If service restarts max_connection will reset to the old value. To make the change permanent, we need to change the configuration file. Find the relevant configuration file. It can be one of below files. (Depends on your installation.) /etc/mysql/mysql.conf.d/mysqld.cnf /etc/mysql/conf.d/mysql.cnf /etc/mysql/mysql.cnf /etc/mysql/my.cnf /etc/my.cnf Once you manage to locate the file, look for the line \u201c max_connections = 151 \u201d and change it to \u201c max_connections = 250 \u201d. Make sure it is not commented. (If there is a # in the begin of that line please remove that #. )","title":"MYSQL General"},{"location":"databases/mysqlgen/#general-mysqlmariadb-commands","text":"","title":"General MYSQL/MARIADB Commands"},{"location":"databases/mysqlgen/#quickly-check-integrity-of-mysql-db","text":"mysqlcheck --all-databases --quick --fast","title":"Quickly check integrity of MySQL DB"},{"location":"databases/mysqlgen/#override-mysql-file-limits","text":"systemctl edit mysqld.service then add this [service] LimitNOFILE=10000","title":"Override MySql file limits"},{"location":"databases/mysqlgen/#change-parameters-in-mysql","text":"Navigate to /etc/my.cnf","title":"Change parameters in MYSQL"},{"location":"databases/mysqlgen/#increase-mysqlmariadb-max_connections-online-without-a-restart","text":"You can use SHOW VARIABLE LIKE 'max_connections'; to check the number of max connections allowed. mysql> SHOW VARIABLE LIKE 'max_connections'; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 151 | +-----------------+-------+ 1 row in set (0.01 sec) You might want to see how many connections you have in your database. You can use the below query to check that. mysql> SELECT COUNT(1) FROM information_schema.processlist; +----------+ | COUNT(1) | +----------+ | 100 | +----------+ 1 row in set (0.00 sec) Let\u2019s assume, we need to increase maximum connections allowed to 250. Please run below query. mysql> SET GLOBAL max_connections = 250; Query OK, 0 rows affected (0.00 sec) You can verify the change by running SHOW VARIABLE LIKE 'max_connections'; . mysql> SHOW VARIABLE LIKE 'max_connections'; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 250 | +-----------------+-------+ 1 row in set (0.01 sec) But our work is not done yet. If service restarts max_connection will reset to the old value. To make the change permanent, we need to change the configuration file. Find the relevant configuration file. It can be one of below files. (Depends on your installation.) /etc/mysql/mysql.conf.d/mysqld.cnf /etc/mysql/conf.d/mysql.cnf /etc/mysql/mysql.cnf /etc/mysql/my.cnf /etc/my.cnf Once you manage to locate the file, look for the line \u201c max_connections = 151 \u201d and change it to \u201c max_connections = 250 \u201d. Make sure it is not commented. (If there is a # in the begin of that line please remove that #. )","title":"Increase MySQL/MariaDB max_connections online (without a restart)"},{"location":"databases/postgres/","text":"Useful postgres Commands Reload config for Postgres Login into database using postgres psql Then run, SELECT pg_reload_conf(); This is when modifying the pg_hba.conf file and you want to apply changes. This won't cause an outage.","title":"Useful Commands"},{"location":"databases/postgres/#useful-postgres-commands","text":"","title":"Useful postgres Commands"},{"location":"databases/postgres/#reload-config-for-postgres","text":"Login into database using postgres psql Then run, SELECT pg_reload_conf(); This is when modifying the pg_hba.conf file and you want to apply changes. This won't cause an outage.","title":"Reload config for Postgres"},{"location":"linux/ad/","text":"Join this host to the usual domain domain and add appropriate yum install adcli oddjob oddjob-mkhomedir realmd samba-common samba-common-tools sssd realm join <local domain> -U join nano /etc/sssd/sssd.conf # SET use_fully_qualified_names = False # SET fallback_homedir = /home/%u systemctl enable sssd systemctl start sssd cat >/etc/sudoers.d/devops <<EOF %<group 1> ALL=(ALL) ALL EOF %<group 2> ALL=(ALL) ALL yum -y install realmd sssd oddjob oddjob-mkhomedir adcli samba-common realm join <local domain> -U *your admin AD account* sed -i 's/use_fully_qualified_names = True/use_fully_qualified_names = False/g' /etc/sssd/sssd.conf systemctl enable sssd systemctl restart sssd Centos 8 Issue If you have an issue on cento s8 with error message like this, Group Policy Container with DN [cn={967D56A2-2F7C-4109-99DC-FDC4040FE9D4},cn=policies,cn=system,DC=<domain>,DC=local] is unreadable or has unreadable or missing attributes. In order to fix this make sure that this AD object has following attributes readable: nTSecurityDescriptor, cn, gPCFileSysPath, gPCMachineExtensionNames, gPCFunctionalityVersion, flags. Alternatively if you do not have access to the server or can not change permissions on this object, you can use option ad_gpo_ignore_unreadable = True which will skip this GPO. See ad_gpo_ignore_unreadable in 'man sssd-ad' for details. then add the following to the /etc/sssd/sssd.conf ad_gpo_ignore_unreadable = True","title":"AD"},{"location":"linux/ad/#join-this-host-to-the-usual-domain-domain-and-add-appropriate","text":"yum install adcli oddjob oddjob-mkhomedir realmd samba-common samba-common-tools sssd realm join <local domain> -U join nano /etc/sssd/sssd.conf # SET use_fully_qualified_names = False # SET fallback_homedir = /home/%u systemctl enable sssd systemctl start sssd cat >/etc/sudoers.d/devops <<EOF %<group 1> ALL=(ALL) ALL EOF %<group 2> ALL=(ALL) ALL yum -y install realmd sssd oddjob oddjob-mkhomedir adcli samba-common realm join <local domain> -U *your admin AD account* sed -i 's/use_fully_qualified_names = True/use_fully_qualified_names = False/g' /etc/sssd/sssd.conf systemctl enable sssd systemctl restart sssd","title":"Join this host to the usual domain domain and add appropriate"},{"location":"linux/ad/#centos-8-issue","text":"If you have an issue on cento s8 with error message like this, Group Policy Container with DN [cn={967D56A2-2F7C-4109-99DC-FDC4040FE9D4},cn=policies,cn=system,DC=<domain>,DC=local] is unreadable or has unreadable or missing attributes. In order to fix this make sure that this AD object has following attributes readable: nTSecurityDescriptor, cn, gPCFileSysPath, gPCMachineExtensionNames, gPCFunctionalityVersion, flags. Alternatively if you do not have access to the server or can not change permissions on this object, you can use option ad_gpo_ignore_unreadable = True which will skip this GPO. See ad_gpo_ignore_unreadable in 'man sssd-ad' for details. then add the following to the /etc/sssd/sssd.conf ad_gpo_ignore_unreadable = True","title":"Centos 8 Issue"},{"location":"linux/bash/","text":"Add time and day stamp to history on Centos Edit /etc/bashrc Then add this line to end of file, export HISTTIMEFORMAT='%F %T '","title":"Bash"},{"location":"linux/bash/#add-time-and-day-stamp-to-history-on-centos","text":"Edit /etc/bashrc Then add this line to end of file, export HISTTIMEFORMAT='%F %T '","title":"Add time and day stamp to history on Centos"},{"location":"linux/cron/","text":"Cron Jobs Schedule automatic jobs in Linux Layout minute hour day of month month day of week Valid entries * any value , value list separator - range of values / step values @yearly @annually @monthly @weekly @daily @hourly @reboot Check Cron for all users This has to be run as root or superuser. for user in $(cut -f1 -d: /etc/passwd); do crontab -u $user -l; done","title":"Cron"},{"location":"linux/cron/#cron-jobs","text":"Schedule automatic jobs in Linux","title":"Cron Jobs"},{"location":"linux/cron/#layout","text":"minute hour day of month month day of week","title":"Layout"},{"location":"linux/cron/#valid-entries","text":"* any value , value list separator - range of values / step values @yearly @annually @monthly @weekly @daily @hourly @reboot","title":"Valid entries"},{"location":"linux/cron/#check-cron-for-all-users","text":"This has to be run as root or superuser. for user in $(cut -f1 -d: /etc/passwd); do crontab -u $user -l; done","title":"Check Cron for all users"},{"location":"linux/docker/","text":"Docker Some stuff I came across while playing with docker. Stop a difficult container Sometimes some containers just won't stop or die when you tell them to, so you use this command and it seems to work. docker kill ----signal=SIGINT <container id> more info here https://www.ctl.io/developers/blog/post/gracefully-stopping-docker-containers/ Docker Commands Portainer Docker cmd docker run -d -p 9000:9000 --restart always --name portainer -v /<volume location>/portainer/data:/data -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer-ce Handling Docker images Save a docker image ready to be deployed to another server. docker save -o <image-name>.tar <image-name> A breakdown of the command docker save -o <image-name> <commited image name> Deploy Docker image to server docker load -i <image-name.tar Docker Scripts Docker cleanup script This script deletes docker containers that haven't being running for a few days. #!/bin/bash # Script to clean up docker containers no longer running after a few days exec 2>&1 { /usr/bin/docker rm $(docker ps -a | grep -v Up\\ | grep '.*Exited\\ .*days\\ .*' | awk '{print $1}') } docker-compose Deploy multiple contianers in a stack using docker-compose, my preferred way of using docker. Firefly docker-compose example --- networks: firefly_iii_net: driver: bridge services: firefly_iii_app: image: jc5x/firefly-iii:latest depends_on: - firefly_iii_db networks: - firefly_iii_net ports: - \"8081:80\" env_file: .env volumes: - source: firefly_iii_export target: /var/www/firefly-iii/storage/export type: volume - source: firefly_iii_upload target: /var/www/firefly-iii/storage/upload type: volume firefly_iii_db: image: \"postgres:10\" environment: - POSTGRES_PASSWORD=secret - POSTGRES_USER=firefly networks: - firefly_iii_net volumes: - firefly_iii_db:/var/lib/postgresql/data version: \"3.2\" volumes: firefly_iii_db: ~ firefly_iii_export: ~ firefly_iii_upload: ~","title":"Docker"},{"location":"linux/docker/#docker","text":"Some stuff I came across while playing with docker.","title":"Docker"},{"location":"linux/docker/#stop-a-difficult-container","text":"Sometimes some containers just won't stop or die when you tell them to, so you use this command and it seems to work. docker kill ----signal=SIGINT <container id> more info here https://www.ctl.io/developers/blog/post/gracefully-stopping-docker-containers/","title":"Stop a difficult container"},{"location":"linux/docker/#docker-commands","text":"","title":"Docker Commands"},{"location":"linux/docker/#portainer-docker-cmd","text":"docker run -d -p 9000:9000 --restart always --name portainer -v /<volume location>/portainer/data:/data -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer-ce","title":"Portainer Docker cmd"},{"location":"linux/docker/#handling-docker-images","text":"Save a docker image ready to be deployed to another server. docker save -o <image-name>.tar <image-name> A breakdown of the command docker save -o <image-name> <commited image name>","title":"Handling Docker images"},{"location":"linux/docker/#deploy-docker-image-to-server","text":"docker load -i <image-name.tar","title":"Deploy Docker image to server"},{"location":"linux/docker/#docker-scripts","text":"","title":"Docker Scripts"},{"location":"linux/docker/#docker-cleanup-script","text":"This script deletes docker containers that haven't being running for a few days. #!/bin/bash # Script to clean up docker containers no longer running after a few days exec 2>&1 { /usr/bin/docker rm $(docker ps -a | grep -v Up\\ | grep '.*Exited\\ .*days\\ .*' | awk '{print $1}') }","title":"Docker cleanup script"},{"location":"linux/docker/#docker-compose","text":"Deploy multiple contianers in a stack using docker-compose, my preferred way of using docker.","title":"docker-compose"},{"location":"linux/docker/#firefly-docker-compose-example","text":"--- networks: firefly_iii_net: driver: bridge services: firefly_iii_app: image: jc5x/firefly-iii:latest depends_on: - firefly_iii_db networks: - firefly_iii_net ports: - \"8081:80\" env_file: .env volumes: - source: firefly_iii_export target: /var/www/firefly-iii/storage/export type: volume - source: firefly_iii_upload target: /var/www/firefly-iii/storage/upload type: volume firefly_iii_db: image: \"postgres:10\" environment: - POSTGRES_PASSWORD=secret - POSTGRES_USER=firefly networks: - firefly_iii_net volumes: - firefly_iii_db:/var/lib/postgresql/data version: \"3.2\" volumes: firefly_iii_db: ~ firefly_iii_export: ~ firefly_iii_upload: ~","title":"Firefly docker-compose example"},{"location":"linux/filesystems/","text":"Common filesytem Operations UUID and storage Find a device UUID and use for say mounting via UUID instead of /dev/sda as an exmample. If you have externald rives, they sometimes get mapped to different /dev/sd? so this is a good way to lock it in. blkid Also you can use, ls -ltr /dev/disk/by-uuid You can also use lsblk by using this command, lsblk -f If you want to use UUID to m ount a drive here is a quick example, UUID=90628bbd-79d0-4455-80d4-2e289f04a7c0 /mnt/tmp auto defaults 0 0 xargs find ./ -name \"*~\" | xargs rm This finds all files in the current directory with the name ending in \"~\" whcih is then piped to xargs which adds each one to its own rm command so effectively passes each file it finds to xargs. sed modifies the contents of files sending the changed file to another file if you so wish sed [options] script-text [input-file] expand converts tab to spaces and unexpand to convert back again. jobs cmd to run to check if there are any jobs running in the shell session.","title":"FileSystems"},{"location":"linux/filesystems/#common-filesytem-operations","text":"","title":"Common filesytem Operations"},{"location":"linux/filesystems/#uuid-and-storage","text":"Find a device UUID and use for say mounting via UUID instead of /dev/sda as an exmample. If you have externald rives, they sometimes get mapped to different /dev/sd? so this is a good way to lock it in. blkid Also you can use, ls -ltr /dev/disk/by-uuid You can also use lsblk by using this command, lsblk -f If you want to use UUID to m ount a drive here is a quick example, UUID=90628bbd-79d0-4455-80d4-2e289f04a7c0 /mnt/tmp auto defaults 0 0","title":"UUID and storage"},{"location":"linux/filesystems/#xargs","text":"find ./ -name \"*~\" | xargs rm This finds all files in the current directory with the name ending in \"~\" whcih is then piped to xargs which adds each one to its own rm command so effectively passes each file it finds to xargs.","title":"xargs"},{"location":"linux/filesystems/#sed","text":"modifies the contents of files sending the changed file to another file if you so wish sed [options] script-text [input-file]","title":"sed"},{"location":"linux/filesystems/#expand","text":"","title":"expand"},{"location":"linux/filesystems/#converts-tab-to-spaces-and-unexpand-to-convert-back-again","text":"","title":"converts tab to spaces and unexpand to convert back again."},{"location":"linux/filesystems/#jobs","text":"","title":"jobs"},{"location":"linux/filesystems/#cmd-to-run-to-check-if-there-are-any-jobs-running-in-the-shell-session","text":"","title":"cmd to run to check if there are any jobs running in the shell session."},{"location":"linux/filesystems/#_1","text":"","title":""},{"location":"linux/firewalld/","text":"firewall-cmd This is the firewall for yum/dnf based distros. Can be installed elsewhere, but examples here are based on RHEL7/8 Allow multiple ports for i in { 8300 8301 8302 8400 8500 8600 } do firewall-cmd --zone=public --add-port=${i}/tcp done firewall-cmd --reload Allow a service firewall-cmd --permanent --add-service https firewall-cmd --reload This adds the ports persistantly, note the --permanent which does this. Block outgoing port on centos/rhel Examples of how to block some outgoing ports firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 0 -p tcp --dport=5225 -j DROP firewall-cmd --reload To remove it firewall-cmd --permanent --direct --remove-rule ipv4 filter OUTPUT 0 -p tcp --dport=5225 -j DROP firewall-cmd --reload","title":"Firewalld"},{"location":"linux/firewalld/#firewall-cmd","text":"This is the firewall for yum/dnf based distros. Can be installed elsewhere, but examples here are based on RHEL7/8","title":"firewall-cmd"},{"location":"linux/firewalld/#allow-multiple-ports","text":"for i in { 8300 8301 8302 8400 8500 8600 } do firewall-cmd --zone=public --add-port=${i}/tcp done firewall-cmd --reload","title":"Allow multiple ports"},{"location":"linux/firewalld/#allow-a-service","text":"firewall-cmd --permanent --add-service https firewall-cmd --reload This adds the ports persistantly, note the --permanent which does this.","title":"Allow a service"},{"location":"linux/firewalld/#block-outgoing-port-on-centosrhel","text":"Examples of how to block some outgoing ports firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 0 -p tcp --dport=5225 -j DROP firewall-cmd --reload To remove it firewall-cmd --permanent --direct --remove-rule ipv4 filter OUTPUT 0 -p tcp --dport=5225 -j DROP firewall-cmd --reload","title":"Block outgoing port on centos/rhel"},{"location":"linux/git/","text":"GIT Commands Ignore error on git pull about locahnges would be overwritten git stash push --include-untracked If you want to drop those changes git stash drop If you want to start over and just do a git pull git reset --hard git pull","title":"GIT"},{"location":"linux/git/#git-commands","text":"","title":"GIT Commands"},{"location":"linux/git/#ignore-error-on-git-pull-about-locahnges-would-be-overwritten","text":"git stash push --include-untracked If you want to drop those changes git stash drop If you want to start over and just do a git pull git reset --hard git pull","title":"Ignore error on git pull about locahnges would be overwritten"},{"location":"linux/logrotate/","text":"Logrotate usage A linux tool to rotate logs automatically so as not to use up all disk spac. Pretty handy. Setup logrotate Run logroate manually For verbose add -v like example below. logrotate -v -f /etc/logrotate.d/httpd Example of logrotate /var/log/httpd/*log { size 200M compress missingok notifempty sharedscripts delaycompress postrotate /bin/systemctl reload httpd.service > /dev/null 2>/dev/null || true endscript } Letsbreak it down. So /var/log/httpd/*log is the location of log file/s. size 200M is size of log file gets before being rotated. compress is basically zipping up the file when rotated. missingok notifempty sharedscripts delaycompress means to compress file on next log rotation. postrotate self explainatory.","title":"Logrotate"},{"location":"linux/logrotate/#logrotate-usage","text":"A linux tool to rotate logs automatically so as not to use up all disk spac. Pretty handy.","title":"Logrotate usage"},{"location":"linux/logrotate/#setup-logrotate","text":"","title":"Setup logrotate"},{"location":"linux/logrotate/#run-logroate-manually","text":"For verbose add -v like example below. logrotate -v -f /etc/logrotate.d/httpd","title":"Run logroate manually"},{"location":"linux/logrotate/#example-of-logrotate","text":"/var/log/httpd/*log { size 200M compress missingok notifempty sharedscripts delaycompress postrotate /bin/systemctl reload httpd.service > /dev/null 2>/dev/null || true endscript } Letsbreak it down. So /var/log/httpd/*log is the location of log file/s. size 200M is size of log file gets before being rotated. compress is basically zipping up the file when rotated. missingok notifempty sharedscripts delaycompress means to compress file on next log rotation. postrotate self explainatory.","title":"Example of logrotate"},{"location":"linux/lvm/","text":"How to use LVM LVM is used to manage disk capacity with Linux. Expand an existing LVM This assumes you have added an extra disk on VMware to the VM affected. echo \"- - -\" > /sys/class/scsi_host/scan0 lsblk fdisk /dev/sdd pvcreate /dev/sdd1 vgs vgextend centos /dev/sdd1 vgs pvscan lvextend -l +100%FREE /dev/centos/var or for specifc sizes, lvextend -L <+size> <Logical volume name path> xfs_growfs /dev/mapper/centos-var Or if filesystem is not XFS then use this instead resize2fs /dev/mapper/centos-var Expand partition for active partition How to expand a live partition. This example uses /dev/sda2 fdisk /dev/sda You will get something like this below, Command (m for help): p Disk /dev/sda: 68.7 GB, 68719476736 bytes, 134217728 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000bffd8 Device Boot Start End Blocks Id System /dev/sda1 * 2048 1050623 524288 83 Linux /dev/sda2 1050624 36716543 17832960 8e Linux LVM So you have checked which one is which, now we delete the partition /dev/sda2 . Command (m for help): d Partition number (1,2, default 2): 2 Partition 2 is deleted Now we create a new partion with a new end block. Command (m for help): n Partition type: p primary (1 primary, 0 extended, 3 free) e extended Select (default p): p Partition number (2-4, default 2): First sector (1050624-134217727, default 1050624): 1050624 Last sector, +sectors or +size{K,M,G} (1050624-134217727, default 134217727): 73433086 Partition 2 of type Linux and of size 34.5 GiB is set Lets check what is now existing Command (m for help): p Disk /dev/sda: 68.7 GB, 68719476736 bytes, 134217728 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000bffd8 Device Boot Start End Blocks Id System /dev/sda1 * 2048 1050623 524288 83 Linux /dev/sda2 1050624 73433086 36191231+ 83 Linux Also, change partition type, using option t and code 8e to change to Linux LVM . Command (m for help): wq The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: Re-reading the partition table failed with error 16: Device or resource busy. The kernel still uses the old table. The new table will be used at the next reboot or after you run partprobe(8) or kpartx(8) Syncing disks. Then run the following partprobe partx -u /dev/sda cat /proc/partitions | grep sda 8 0 67108864 sda 8 1 524288 sda1 8 2 36191231 sda2 Now run pvresize /dev/sda2 You'll get this below Physical volume \"/dev/sda2\" changed 1 physical volume(s) resized or updated / 0 physical volume(s) not resized Now run pvs PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- 34.51g 17.51g Parted Select disk parted (parted) select /dev/sda To see partitions (parted) print This defines START and END of disk locations. (parted) mkpart primary 106 16179 You can enable boot option on partition. (parted) set 1 boot on Logical partition with 127GB (parted) mkpart logical 372737 500000 Create a file system on Partition using mkfs (parted) mkfs Create Parition and filesystem together using mkpartfs (parted) mkpartfs logical fat32 372737 500000 Resize partiiton from one size to another using resize (parted) resize 9","title":"LVM"},{"location":"linux/lvm/#how-to-use-lvm","text":"LVM is used to manage disk capacity with Linux.","title":"How to use LVM"},{"location":"linux/lvm/#expand-an-existing-lvm","text":"This assumes you have added an extra disk on VMware to the VM affected. echo \"- - -\" > /sys/class/scsi_host/scan0 lsblk fdisk /dev/sdd pvcreate /dev/sdd1 vgs vgextend centos /dev/sdd1 vgs pvscan lvextend -l +100%FREE /dev/centos/var or for specifc sizes, lvextend -L <+size> <Logical volume name path> xfs_growfs /dev/mapper/centos-var Or if filesystem is not XFS then use this instead resize2fs /dev/mapper/centos-var","title":"Expand an existing LVM"},{"location":"linux/lvm/#expand-partition-for-active-partition","text":"How to expand a live partition. This example uses /dev/sda2 fdisk /dev/sda You will get something like this below, Command (m for help): p Disk /dev/sda: 68.7 GB, 68719476736 bytes, 134217728 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000bffd8 Device Boot Start End Blocks Id System /dev/sda1 * 2048 1050623 524288 83 Linux /dev/sda2 1050624 36716543 17832960 8e Linux LVM So you have checked which one is which, now we delete the partition /dev/sda2 . Command (m for help): d Partition number (1,2, default 2): 2 Partition 2 is deleted Now we create a new partion with a new end block. Command (m for help): n Partition type: p primary (1 primary, 0 extended, 3 free) e extended Select (default p): p Partition number (2-4, default 2): First sector (1050624-134217727, default 1050624): 1050624 Last sector, +sectors or +size{K,M,G} (1050624-134217727, default 134217727): 73433086 Partition 2 of type Linux and of size 34.5 GiB is set Lets check what is now existing Command (m for help): p Disk /dev/sda: 68.7 GB, 68719476736 bytes, 134217728 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000bffd8 Device Boot Start End Blocks Id System /dev/sda1 * 2048 1050623 524288 83 Linux /dev/sda2 1050624 73433086 36191231+ 83 Linux Also, change partition type, using option t and code 8e to change to Linux LVM . Command (m for help): wq The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: Re-reading the partition table failed with error 16: Device or resource busy. The kernel still uses the old table. The new table will be used at the next reboot or after you run partprobe(8) or kpartx(8) Syncing disks. Then run the following partprobe partx -u /dev/sda cat /proc/partitions | grep sda 8 0 67108864 sda 8 1 524288 sda1 8 2 36191231 sda2 Now run pvresize /dev/sda2 You'll get this below Physical volume \"/dev/sda2\" changed 1 physical volume(s) resized or updated / 0 physical volume(s) not resized Now run pvs PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- 34.51g 17.51g","title":"Expand partition for active partition"},{"location":"linux/lvm/#parted","text":"Select disk parted (parted) select /dev/sda To see partitions (parted) print This defines START and END of disk locations. (parted) mkpart primary 106 16179","title":"Parted"},{"location":"linux/lvm/#you-can-enable-boot-option-on-partition","text":"(parted) set 1 boot on","title":"You can enable boot option on partition."},{"location":"linux/lvm/#logical-partition-with-127gb","text":"(parted) mkpart logical 372737 500000","title":"Logical partition with 127GB"},{"location":"linux/lvm/#create-a-file-system-on-partition-using-mkfs","text":"(parted) mkfs","title":"Create a file system on Partition using mkfs"},{"location":"linux/lvm/#create-parition-and-filesystem-together-using-mkpartfs","text":"(parted) mkpartfs logical fat32 372737 500000","title":"Create Parition and filesystem together using mkpartfs"},{"location":"linux/lvm/#resize-partiiton-from-one-size-to-another-using-resize","text":"(parted) resize 9","title":"Resize partiiton from one size to another using resize"},{"location":"linux/mail/","text":"Mailx Send mail with an attachment mail -s \"Subject line of email\" -r \"email address of sender\" -a <location of file> <recipient addresses> < /dev/null","title":"Mail"},{"location":"linux/mail/#mailx","text":"","title":"Mailx"},{"location":"linux/mail/#send-mail-with-an-attachment","text":"mail -s \"Subject line of email\" -r \"email address of sender\" -a <location of file> <recipient addresses> < /dev/null","title":"Send mail with an attachment"},{"location":"linux/nano/","text":"How to delete or move blocks use CTRL+Shift+6 to mark the beginning of your block move cursor with arrow keys to end of your block, the text will be highlighted. use CTRL+K to cut/delete block. To paste the block to another place, move cursor to the position and the use CTRL+U . You can paste the block as often as you want to.","title":"Nano"},{"location":"linux/nano/#how-to-delete-or-move-blocks","text":"use CTRL+Shift+6 to mark the beginning of your block move cursor with arrow keys to end of your block, the text will be highlighted. use CTRL+K to cut/delete block. To paste the block to another place, move cursor to the position and the use CTRL+U . You can paste the block as often as you want to.","title":"How to delete or move blocks"},{"location":"linux/networking/","text":"Setup Static IP on Debian 10/11 Edit this file, nano /etc/network/interfaces Add the following iface ens18 inet static address <ip address> netmask 255.255.255.0 gateway <gateway ip, typically router> dns-domain <domain name> dns-nameservers <ip of dns server> Restart networking service systemctl restart networking.service","title":"Networking"},{"location":"linux/networking/#setup-static-ip-on-debian-1011","text":"Edit this file, nano /etc/network/interfaces Add the following iface ens18 inet static address <ip address> netmask 255.255.255.0 gateway <gateway ip, typically router> dns-domain <domain name> dns-nameservers <ip of dns server> Restart networking service systemctl restart networking.service","title":"Setup Static IP on Debian 10/11"},{"location":"linux/podman/","text":"Podman Podman is Redhats answer to Docker. You can un docker containers as a systemd service. Bind podman container to local directory sudo podman run -it -p 80:80 --name nginx-proxy -v /opt/podman/nginx/conf.d:/etc/nginx/conf.d nginx sh Install Podman on RHEL 9 Run the following to install dnf install git dnf install python3-pip pip3 install --upgrade pip pip3 install podman-compose Some basic commands for podman-compose To get list of commands just run podman-compose Most commands are like using docker-compose when you have a docker-compose file. So you can create a docker-compose file and then run, podman-compose pull podman-compose up or podman-compose up -d","title":"Podman"},{"location":"linux/podman/#podman","text":"Podman is Redhats answer to Docker. You can un docker containers as a systemd service.","title":"Podman"},{"location":"linux/podman/#bind-podman-container-to-local-directory","text":"sudo podman run -it -p 80:80 --name nginx-proxy -v /opt/podman/nginx/conf.d:/etc/nginx/conf.d nginx sh","title":"Bind podman container to local directory"},{"location":"linux/podman/#install-podman-on-rhel-9","text":"Run the following to install dnf install git dnf install python3-pip pip3 install --upgrade pip pip3 install podman-compose","title":"Install Podman on RHEL 9"},{"location":"linux/podman/#some-basic-commands-for-podman-compose","text":"To get list of commands just run podman-compose Most commands are like using docker-compose when you have a docker-compose file. So you can create a docker-compose file and then run, podman-compose pull podman-compose up or podman-compose up -d","title":"Some basic commands for podman-compose"},{"location":"linux/raspberry-pi/","text":"Useful stuff for Raspberry Pi Show GPU temperature vcgencmd measure_temp OR /opt/vc/bin/vcgencmd measure_temp Show CPU temperature cat /sys/class/thermal/thermal_zone0/temp Divide it by 1000 to see the temperature in a more readable cpu=$(</sys/class/thermal/thermal_zone0/temp) echo \"$((cpu/1000)) c\" A script to put it togther #!/bin/bash cpu=(</sys/class/thermal/thermal_zone0/temp) echo \"(</sys/class/thermal/thermalzone0/temp)echo\"(date) @ $(hostname)\" echo \"-------------------------------------------\" echo \"GPU => $(/opt/vc/bin/vcgencmd measure_temp)\" echo \"CPU => $((cpu/1000))'C\" Configure a Static IP address","title":"Pi"},{"location":"linux/raspberry-pi/#useful-stuff-for-raspberry-pi","text":"","title":"Useful stuff for Raspberry Pi"},{"location":"linux/raspberry-pi/#show-gpu-temperature","text":"vcgencmd measure_temp OR /opt/vc/bin/vcgencmd measure_temp","title":"Show GPU temperature"},{"location":"linux/raspberry-pi/#show-cpu-temperature","text":"cat /sys/class/thermal/thermal_zone0/temp Divide it by 1000 to see the temperature in a more readable cpu=$(</sys/class/thermal/thermal_zone0/temp) echo \"$((cpu/1000)) c\" A script to put it togther #!/bin/bash cpu=(</sys/class/thermal/thermal_zone0/temp) echo \"(</sys/class/thermal/thermalzone0/temp)echo\"(date) @ $(hostname)\" echo \"-------------------------------------------\" echo \"GPU => $(/opt/vc/bin/vcgencmd measure_temp)\" echo \"CPU => $((cpu/1000))'C\"","title":"Show CPU temperature"},{"location":"linux/raspberry-pi/#configure-a-static-ip-address","text":"","title":"Configure a Static IP address"},{"location":"linux/redhat/","text":"REDHAT Specific Subscribe REDHAT VM to subscription There are a few different options, but what I do is change hostname to what I want then do this, subscription-manager register --force --username --auto-attach This removes if already registered and resubscribes with auth attachment so you don;t have to go into UI to do it or eun from CLI. You get an issue like EE crypto too weak. does ths, update-crypto-policies --show update-crypto-policies --set DEFAULT to make updated system-wide crypto policy active update-crypto-policies usually set to FUTURE Docker conflicts When installing docker there are conflicts with podman and containerd yum erase podman buildah Get detailed stats on a file stat -L <absolute path to file> Check for updates etc and if a system reboot is required dnf check-update dnf needs-restarting -r SSSD service is failing SSSD service is failing with an error 'Failed to initialize credentials using keytab [MEMORY:/etc/krb5.keytab]: Preauthentication failed.' Environment Red Hat Enterprise Linux 7 Red Hat Enterprise Linux 8 Issue SSSD service is failing. RHEL system is configured as an AD client using SSSD and AD users are unable to login to the system. /var/log/messages file is filled up with following repeated logs. Mar 13 08:36:18 testserver [sssd[ldap_child[145919]]]: Failed to initialize credentials using keytab [MEMORY:/etc/krb5.keytab]: Preauthentication failed. Unable to create GSSAPI-encrypted LDAP connection. Resolution Active Directory Client RHEL 7/8 Take a backup of existing /etc/sssd/sssd.conf file: cp /etc/sssd/sssd.conf /tmp/sssd.bak Then remove the system from domain using realm command as: Make sure the old Keytab is deleted: Join the system to AD domain again. How to join RHEL to Active Directory using realmd Root Cause The following log indicates that the /etc/krb5.keytab file has become invalid. Failed to initialize credentials using keytab [MEMORY:/etc/krb5.keytab]: Preauthentication failed. Unable to create GSSAPI-encrypted LDAP connection. Diagnostic Steps Get sosreport from system in question. Check sssd.conf file. Check error in /var/log/messages file.","title":"RHEL"},{"location":"linux/redhat/#redhat-specific","text":"","title":"REDHAT Specific"},{"location":"linux/redhat/#subscribe-redhat-vm-to-subscription","text":"There are a few different options, but what I do is change hostname to what I want then do this, subscription-manager register --force --username --auto-attach This removes if already registered and resubscribes with auth attachment so you don;t have to go into UI to do it or eun from CLI. You get an issue like EE crypto too weak. does ths, update-crypto-policies --show update-crypto-policies --set DEFAULT to make updated system-wide crypto policy active update-crypto-policies usually set to FUTURE","title":"Subscribe REDHAT VM to subscription"},{"location":"linux/redhat/#docker-conflicts","text":"When installing docker there are conflicts with podman and containerd yum erase podman buildah","title":"Docker conflicts"},{"location":"linux/redhat/#get-detailed-stats-on-a-file","text":"stat -L <absolute path to file>","title":"Get detailed stats on a file"},{"location":"linux/redhat/#check-for-updates-etc-and-if-a-system-reboot-is-required","text":"dnf check-update dnf needs-restarting -r","title":"Check for updates etc and if a system reboot is required"},{"location":"linux/redhat/#sssd-service-is-failing","text":"SSSD service is failing with an error 'Failed to initialize credentials using keytab [MEMORY:/etc/krb5.keytab]: Preauthentication failed.'","title":"SSSD service is failing"},{"location":"linux/redhat/#environment","text":"Red Hat Enterprise Linux 7 Red Hat Enterprise Linux 8","title":"Environment"},{"location":"linux/redhat/#issue","text":"SSSD service is failing. RHEL system is configured as an AD client using SSSD and AD users are unable to login to the system. /var/log/messages file is filled up with following repeated logs. Mar 13 08:36:18 testserver [sssd[ldap_child[145919]]]: Failed to initialize credentials using keytab [MEMORY:/etc/krb5.keytab]: Preauthentication failed. Unable to create GSSAPI-encrypted LDAP connection.","title":"Issue"},{"location":"linux/redhat/#resolution","text":"Active Directory Client RHEL 7/8 Take a backup of existing /etc/sssd/sssd.conf file: cp /etc/sssd/sssd.conf /tmp/sssd.bak Then remove the system from domain using realm command as: Make sure the old Keytab is deleted: Join the system to AD domain again. How to join RHEL to Active Directory using realmd","title":"Resolution"},{"location":"linux/redhat/#root-cause","text":"The following log indicates that the /etc/krb5.keytab file has become invalid. Failed to initialize credentials using keytab [MEMORY:/etc/krb5.keytab]: Preauthentication failed. Unable to create GSSAPI-encrypted LDAP connection.","title":"Root Cause"},{"location":"linux/redhat/#diagnostic-steps","text":"Get sosreport from system in question. Check sssd.conf file. Check error in /var/log/messages file.","title":"Diagnostic Steps"},{"location":"linux/swap/","text":"All things Swap Check Swappiness cat /proc/sys/vm/swappiness To set lower swappiness sysctl vm.swappiness=10 to make persistent, add to /etc/sysctl.conf vm.swappiness=10 How to display processes using swap space Display processes using swap space find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") printf \"%10s %-30s %20s\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"]}' '{}' \\; Display processes using swap space sorted by used space find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") printf \"%10s %-30s %20s\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"]}' '{}' \\; | awk '{print $(NF-1),$0}' | sort -h | cut -d \" \" -f2- Display top ten processes using swap space find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") printf \"%10s %-30s %20s\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"]}' '{}' \\; | awk '{print $(NF-1),$0}' | sort -hr | head | cut -d \" \" -f2- Display top ten processes using swap spac with percentage values To read find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" -v TOTSWP=\"$(cat /proc/meminfo | sed -n -e \"s/^SwapTotal:[ ]*\\([0-9]*\\) kB/\\1/p\")\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") {used_swap=process[\"VmSwap\"];sub(/[ a-zA-Z]+/,\"\",used_swap);percent=(used_swap/TOTSWP*100); printf \"%10s %-30s %20s %6.2f%\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"],percent} }' '{}' \\; | awk '{print $(NF-2),$0}' | sort -hr | head | cut -d \" \" -f2- To calculate find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" -v TOTSWP=\"$(cat /proc/swaps | sed 1d | awk 'BEGIN{sum=0} {sum=sum+$(NF-2)} END{print sum}')\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") {used_swap=process[\"VmSwap\"];sub(/[ a-zA-Z]+/,\"\",used_swap);percent=(used_swap/TOTSWP*100); printf \"%10s %-30s %20s %6.2f%\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"],percent} }' '{}' \\; | awk '{print $(NF-2),$0}' | sort -hr | head | cut -d \" \" -f2- Check which processes are using swap for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file; done | sort -k 2 -n -r | less","title":"Swap"},{"location":"linux/swap/#all-things-swap","text":"","title":"All things Swap"},{"location":"linux/swap/#check-swappiness","text":"cat /proc/sys/vm/swappiness To set lower swappiness sysctl vm.swappiness=10 to make persistent, add to /etc/sysctl.conf vm.swappiness=10","title":"Check Swappiness"},{"location":"linux/swap/#how-to-display-processes-using-swap-space","text":"","title":"How to display processes using swap space"},{"location":"linux/swap/#display-processes-using-swap-space","text":"find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") printf \"%10s %-30s %20s\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"]}' '{}' \\;","title":"Display processes using swap space"},{"location":"linux/swap/#display-processes-using-swap-space-sorted-by-used-space","text":"find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") printf \"%10s %-30s %20s\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"]}' '{}' \\; | awk '{print $(NF-1),$0}' | sort -h | cut -d \" \" -f2-","title":"Display processes using swap space sorted by used space"},{"location":"linux/swap/#display-top-ten-processes-using-swap-space","text":"find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") printf \"%10s %-30s %20s\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"]}' '{}' \\; | awk '{print $(NF-1),$0}' | sort -hr | head | cut -d \" \" -f2-","title":"Display top ten processes using swap space"},{"location":"linux/swap/#display-top-ten-processes-using-swap-spac-with-percentage-values","text":"To read find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" -v TOTSWP=\"$(cat /proc/meminfo | sed -n -e \"s/^SwapTotal:[ ]*\\([0-9]*\\) kB/\\1/p\")\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") {used_swap=process[\"VmSwap\"];sub(/[ a-zA-Z]+/,\"\",used_swap);percent=(used_swap/TOTSWP*100); printf \"%10s %-30s %20s %6.2f%\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"],percent} }' '{}' \\; | awk '{print $(NF-2),$0}' | sort -hr | head | cut -d \" \" -f2- To calculate find /proc -maxdepth 2 -path \"/proc/[0-9]*/status\" -readable -exec awk -v FS=\":\" -v TOTSWP=\"$(cat /proc/swaps | sed 1d | awk 'BEGIN{sum=0} {sum=sum+$(NF-2)} END{print sum}')\" '{process[$1]=$2;sub(/^[ \\t]+/,\"\",process[$1]);} END {if(process[\"VmSwap\"] && process[\"VmSwap\"] != \"0 kB\") {used_swap=process[\"VmSwap\"];sub(/[ a-zA-Z]+/,\"\",used_swap);percent=(used_swap/TOTSWP*100); printf \"%10s %-30s %20s %6.2f%\\n\",process[\"Pid\"],process[\"Name\"],process[\"VmSwap\"],percent} }' '{}' \\; | awk '{print $(NF-2),$0}' | sort -hr | head | cut -d \" \" -f2-","title":"Display top ten processes using swap spac with percentage values"},{"location":"linux/swap/#check-which-processes-are-using-swap","text":"for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file; done | sort -k 2 -n -r | less","title":"Check which processes are using swap"},{"location":"linux/useful/","text":"Useful Commands for General Linux Use GZIP on the fly with tar tar cvf - FILE-LIST | gzip -c > FILE.tar.gz Find file count find . -xdev -type f | cut -d \"/\" -f 2 | sort | uniq -c | sort -n PS ps -w changes so the display is wide and not limited to 80 columns and not truncated Special Permissions Sticky bits rwxr-xr-t Allows only the owner to delete the files in that directory.Protects files from been deleted by anyone who is not the owner. Would be used for shared folders, instances like that. Set user id's In conjunction with executable files, so run the program with the permissions of the file owners, NOT the permissions of whoever runs it. SUID root Set Group ID Check for hung processes strace -p <pid> Checks i/o calls from application, if nothing happens it is probably hung. Shutdown remotely shutdown -f -r -m <ipaddress> -t 00 Zombie Processes pstree -ch root | more Don\u2019t use kill -9 to kill the parent process, it does not always allow enough time to kill it cleanly. Just use kill Check Available memory on Linux To check available memory in Linux type free -m command. free displays the total amount of free and used physical and swap memory in the system, as well as the buffers used by the kernel. Command: free -m free -h free -b free -g Output: total used free shared buffers cached Mem: 2025 1961 64 0 172 1035 -/ buffers/cache: 753 1272 Swap: 1906 0 1906 Description: The -b switch displays the amount of memory in bytes The -k switch (set by default) displays it in kilobytes The -m switch displays it in megabytes The -g switch displays it in gigabytes. Command: vmstat Output: procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---- r b swpd free buff cache si so bi bo in cs us sy id wa 0 0 0 63316 176624 1062340 0 0 25 18 39 592 5 1 94 0 Description: swpd: the amount of virtual memory used. free: the amount of idle memory. buff: the amount of memory used as buffers. cache: the amount of memory used as cache. Symlinks Create a Symlink The below example is a softlink ln -s <path to the file/folder to be linked> <the path of the link to be created> Delete a Symlink unlink <path-to-symlink> Find Remove folders less than specified parameter find . -mindepth 1 -maxdepth 1 -type d -exec du -ks {} + | awk '$1 <= 250' | cut -f 2- | xargs -d \\\\n rm -rf to just see results, then remove below form command above. | xargs -d \\\\n rm -rf","title":"Useful Commands"},{"location":"linux/useful/#useful-commands-for-general-linux-use","text":"","title":"Useful Commands for General Linux Use"},{"location":"linux/useful/#gzip-on-the-fly-with-tar","text":"tar cvf - FILE-LIST | gzip -c > FILE.tar.gz","title":"GZIP on the fly with tar"},{"location":"linux/useful/#find-file-count","text":"find . -xdev -type f | cut -d \"/\" -f 2 | sort | uniq -c | sort -n","title":"Find file count"},{"location":"linux/useful/#ps","text":"ps -w","title":"PS"},{"location":"linux/useful/#changes-so-the-display-is-wide-and-not-limited-to-80-columns-and-not-truncated","text":"","title":"changes so the display is wide and not limited to 80 columns and not truncated"},{"location":"linux/useful/#special-permissions","text":"Sticky bits rwxr-xr-t Allows only the owner to delete the files in that directory.Protects files from been deleted by anyone who is not the owner. Would be used for shared folders, instances like that. Set user id's In conjunction with executable files, so run the program with the permissions of the file owners, NOT the permissions of whoever runs it. SUID root Set Group ID","title":"Special Permissions"},{"location":"linux/useful/#check-for-hung-processes","text":"strace -p <pid> Checks i/o calls from application, if nothing happens it is probably hung.","title":"Check for hung processes"},{"location":"linux/useful/#shutdown-remotely","text":"shutdown -f -r -m <ipaddress> -t 00","title":"Shutdown remotely"},{"location":"linux/useful/#zombie-processes","text":"pstree -ch root | more Don\u2019t use kill -9 to kill the parent process, it does not always allow enough time to kill it cleanly. Just use kill","title":"Zombie Processes"},{"location":"linux/useful/#check-available-memory-on-linux","text":"To check available memory in Linux type free -m command. free displays the total amount of free and used physical and swap memory in the system, as well as the buffers used by the kernel. Command: free -m free -h free -b free -g Output: total used free shared buffers cached Mem: 2025 1961 64 0 172 1035 -/ buffers/cache: 753 1272 Swap: 1906 0 1906 Description: The -b switch displays the amount of memory in bytes The -k switch (set by default) displays it in kilobytes The -m switch displays it in megabytes The -g switch displays it in gigabytes. Command: vmstat Output: procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---- r b swpd free buff cache si so bi bo in cs us sy id wa 0 0 0 63316 176624 1062340 0 0 25 18 39 592 5 1 94 0 Description: swpd: the amount of virtual memory used. free: the amount of idle memory. buff: the amount of memory used as buffers. cache: the amount of memory used as cache.","title":"Check Available memory on Linux"},{"location":"linux/useful/#symlinks","text":"","title":"Symlinks"},{"location":"linux/useful/#create-a-symlink","text":"The below example is a softlink ln -s <path to the file/folder to be linked> <the path of the link to be created>","title":"Create a Symlink"},{"location":"linux/useful/#delete-a-symlink","text":"unlink <path-to-symlink>","title":"Delete a Symlink"},{"location":"linux/useful/#find","text":"","title":"Find"},{"location":"linux/useful/#remove-folders-less-than-specified-parameter","text":"find . -mindepth 1 -maxdepth 1 -type d -exec du -ks {} + | awk '$1 <= 250' | cut -f 2- | xargs -d \\\\n rm -rf to just see results, then remove below form command above. | xargs -d \\\\n rm -rf","title":"Remove folders less than specified parameter"},{"location":"monitoring/prometheus/","text":"All things Prometheus How to install node-exporter A node-exporter is effectively an agent which talks to Prometheus server to send metrics to it from a host. Here is how to set it up. Download latest node-exporter wget -O node-exporter.tar.gz https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-armv7.tar.gz tar -xvf node-exporter.tar.gz --strip-components=1 I usually put this is /opt Now setup as service, make sure you have a user existing for this service. Setup user without login useradd -s /sbin/nologin prometheus Create systemctl service nano /etc/systemd/system/nodeexporter.service [Unit] Description=Prometheus Node Exporter Documentation=https://prometheus.io/docs/guides/node-exporter/ After=network-online.target [Service] User=prometheus Restart=on-failure ExecStart=/opt/node-exporter/node_exporter [Install] WantedBy=multi-user.target systemctl daemon-reload","title":"Prometheus"},{"location":"monitoring/prometheus/#all-things-prometheus","text":"","title":"All things Prometheus"},{"location":"monitoring/prometheus/#how-to-install-node-exporter","text":"A node-exporter is effectively an agent which talks to Prometheus server to send metrics to it from a host. Here is how to set it up. Download latest node-exporter wget -O node-exporter.tar.gz https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-armv7.tar.gz tar -xvf node-exporter.tar.gz --strip-components=1 I usually put this is /opt Now setup as service, make sure you have a user existing for this service. Setup user without login useradd -s /sbin/nologin prometheus Create systemctl service nano /etc/systemd/system/nodeexporter.service [Unit] Description=Prometheus Node Exporter Documentation=https://prometheus.io/docs/guides/node-exporter/ After=network-online.target [Service] User=prometheus Restart=on-failure ExecStart=/opt/node-exporter/node_exporter [Install] WantedBy=multi-user.target systemctl daemon-reload","title":"How to install node-exporter"},{"location":"networks/haproxy/","text":"HAProxy Haproxy is a kick ass proxy which is scalable and has a multitude of uses. HAProxy Example An example config of HAProxy that is a tcp pass through. #--------------------------------------------------------------------- # Example configuration for a possible web application. See the # full configuration options online. # # https://www.haproxy.org/download/1.8/doc/configuration.txt # #--------------------------------------------------------------------- #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global # to have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the '-r' option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats # utilize system-wide crypto-policies ssl-default-bind-ciphers PROFILE=SYSTEM ssl-default-server-ciphers PROFILE=SYSTEM #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 #--------------------------------------------------------------------- # main frontend which proxys to the backends #--------------------------------------------------------------------- #frontend main # bind *:5000 # acl url_static path_beg -i /static /images /javascript /stylesheets # acl url_static path_end -i .jpg .gif .png .css .js # use_backend static if url_static # default_backend app frontend https_in mode tcp option tcplog bind *:443 acl tls req.ssl_hello_type 1 tcp-request inspect-delay 5s tcp-request content accept if tls acl <server1> req.ssl_sni -i <sub domain name> acl <server1> req.ssl_sni -i <sub domain name> acl <server2> req.ssl_sni -i <sub domain name> acl <server2> req.ssl_sni -i <sub domain name> use_backend <server1> if <server1> use_backend <server2> if <server2> #--------------------------------------------------------------------- # static backend for serving up images, stylesheets and such #--------------------------------------------------------------------- #backend static # balance roundrobin # server static 127.0.0.1:4331 check backend <server1> mode tcp option tcplog option ssl-hello-chk server <server1> <ip address of backend server>:<port> backend <server2> mode tcp option tcplog option ssl-hello-chk server <server2> <ip address of backend server>:<port> #--------------------------------------------------------------------- # round robin balancing between the various backends #--------------------------------------------------------------------- #backend app # balance roundrobin # server app1 127.0.0.1:5001 check # server app2 127.0.0.1:5002 check # server app3 127.0.0.1:5003 check # server app4 127.0.0.1:5004 check The Stats Section This is a stats page you can login to, to view the HAProxy stats. mode http bind <ip address of host>:<port> # ssl crt /etc/ssl/<cert chain - pem format> stats enable stats show-legends stats hide-version stats realm Haproxy\\ Statistics stats uri /stats stats refresh 5s stats auth <username>:<password> Another example Here is an example of frontends listening on different ports and implementing STunnel to enccrpt traffic.","title":"HAProxy"},{"location":"networks/haproxy/#haproxy","text":"Haproxy is a kick ass proxy which is scalable and has a multitude of uses.","title":"HAProxy"},{"location":"networks/haproxy/#haproxy-example","text":"An example config of HAProxy that is a tcp pass through. #--------------------------------------------------------------------- # Example configuration for a possible web application. See the # full configuration options online. # # https://www.haproxy.org/download/1.8/doc/configuration.txt # #--------------------------------------------------------------------- #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global # to have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the '-r' option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats # utilize system-wide crypto-policies ssl-default-bind-ciphers PROFILE=SYSTEM ssl-default-server-ciphers PROFILE=SYSTEM #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 #--------------------------------------------------------------------- # main frontend which proxys to the backends #--------------------------------------------------------------------- #frontend main # bind *:5000 # acl url_static path_beg -i /static /images /javascript /stylesheets # acl url_static path_end -i .jpg .gif .png .css .js # use_backend static if url_static # default_backend app frontend https_in mode tcp option tcplog bind *:443 acl tls req.ssl_hello_type 1 tcp-request inspect-delay 5s tcp-request content accept if tls acl <server1> req.ssl_sni -i <sub domain name> acl <server1> req.ssl_sni -i <sub domain name> acl <server2> req.ssl_sni -i <sub domain name> acl <server2> req.ssl_sni -i <sub domain name> use_backend <server1> if <server1> use_backend <server2> if <server2> #--------------------------------------------------------------------- # static backend for serving up images, stylesheets and such #--------------------------------------------------------------------- #backend static # balance roundrobin # server static 127.0.0.1:4331 check backend <server1> mode tcp option tcplog option ssl-hello-chk server <server1> <ip address of backend server>:<port> backend <server2> mode tcp option tcplog option ssl-hello-chk server <server2> <ip address of backend server>:<port> #--------------------------------------------------------------------- # round robin balancing between the various backends #--------------------------------------------------------------------- #backend app # balance roundrobin # server app1 127.0.0.1:5001 check # server app2 127.0.0.1:5002 check # server app3 127.0.0.1:5003 check # server app4 127.0.0.1:5004 check","title":"HAProxy Example"},{"location":"networks/haproxy/#the-stats-section","text":"This is a stats page you can login to, to view the HAProxy stats. mode http bind <ip address of host>:<port> # ssl crt /etc/ssl/<cert chain - pem format> stats enable stats show-legends stats hide-version stats realm Haproxy\\ Statistics stats uri /stats stats refresh 5s stats auth <username>:<password>","title":"The Stats Section"},{"location":"networks/haproxy/#another-example","text":"Here is an example of frontends listening on different ports and implementing STunnel to enccrpt traffic.","title":"Another example"},{"location":"networks/nginx/","text":"Nginx conf example for Bitwarden This example points at a docker container running on the same host. server { server_name example.domain.com; location / { proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-Protocol $scheme; proxy_set_header X-Url-Scheme $scheme; proxy_pass http://127.0.0.1:8080/; client_max_body_size 0; } listen 443 ssl; # managed by Certbot ssl_certificate /etc/letsencrypt/live/example.domain.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/example.domain.com/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot } server { if ($host = example.domain.com) { return 301 https://$host$request_uri; } # managed by Certbot server_name example.domain.com; listen 80; return 404; # managed by Certbot","title":"Nginx"},{"location":"networks/nginx/#_1","text":"","title":""},{"location":"networks/nginx/#nginx-conf-example-for-bitwarden","text":"This example points at a docker container running on the same host. server { server_name example.domain.com; location / { proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-Protocol $scheme; proxy_set_header X-Url-Scheme $scheme; proxy_pass http://127.0.0.1:8080/; client_max_body_size 0; } listen 443 ssl; # managed by Certbot ssl_certificate /etc/letsencrypt/live/example.domain.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/example.domain.com/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot } server { if ($host = example.domain.com) { return 301 https://$host$request_uri; } # managed by Certbot server_name example.domain.com; listen 80; return 404; # managed by Certbot","title":"Nginx conf example for Bitwarden"},{"location":"networks/stunnel/","text":"Stunnel Stunnel is a free linux program to encrypt traffic for any application that does not have encrypted traffic. You basically redirect that applications traffic to the Stunnel port and it will encrypt it across the network. Config Here is an example configuration. /etc/stunnel/stunnel.conf cert = /etc/pki/tls/certs/stunnel.pem ; Allow only TLS, thus avoiding SSL #sslVersion = all options = NO_SSLv2 chroot = /var/lib/stunnel setuid = nobody setgid = nobody pid = /stunnel.pid socket = l:TCP_NODELAY=1 socket = r:TCP_NODELAY=1 #foreground = no client = yes debug = 7 output = /log/stunnel.log [<name of app to encrypt, ie metricbeat>] accept = localhost:5220 connect = <ip address of destination>:<port number> TIMEOUTclose = 0 verify = 0 Example [metricbeat] accept = localhost:5220 connect = 192.168.1.10:5225 TIMEOUTclose = 0 verify = 0 This is slightly different to the standard config as its in /var/lib/stunnel not /var/run/stunnel . This is to get around the issue when server reboots the /var/run/stunnel * chroot gets wiped.","title":"Stunnel"},{"location":"networks/stunnel/#stunnel","text":"Stunnel is a free linux program to encrypt traffic for any application that does not have encrypted traffic. You basically redirect that applications traffic to the Stunnel port and it will encrypt it across the network.","title":"Stunnel"},{"location":"networks/stunnel/#config","text":"Here is an example configuration. /etc/stunnel/stunnel.conf cert = /etc/pki/tls/certs/stunnel.pem ; Allow only TLS, thus avoiding SSL #sslVersion = all options = NO_SSLv2 chroot = /var/lib/stunnel setuid = nobody setgid = nobody pid = /stunnel.pid socket = l:TCP_NODELAY=1 socket = r:TCP_NODELAY=1 #foreground = no client = yes debug = 7 output = /log/stunnel.log [<name of app to encrypt, ie metricbeat>] accept = localhost:5220 connect = <ip address of destination>:<port number> TIMEOUTclose = 0 verify = 0","title":"Config"},{"location":"networks/stunnel/#example","text":"[metricbeat] accept = localhost:5220 connect = 192.168.1.10:5225 TIMEOUTclose = 0 verify = 0 This is slightly different to the standard config as its in /var/lib/stunnel not /var/run/stunnel . This is to get around the issue when server reboots the /var/run/stunnel * chroot gets wiped.","title":"Example"},{"location":"networks/zerotier/","text":"How to install Zerotier Basic install curl -s https://install.zerotier.com | sudo bash More secure curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \\ if z=$(curl -s 'https://install.zerotier.com/' | gpg); then echo \"$z\" | sudo bash; fi To Join a network From the command line simply type zerotier-cli join ################ with ############### being the 16-digit network ID of the network you wish to join.","title":"Zerotier"},{"location":"networks/zerotier/#how-to-install-zerotier","text":"Basic install curl -s https://install.zerotier.com | sudo bash More secure curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \\ if z=$(curl -s 'https://install.zerotier.com/' | gpg); then echo \"$z\" | sudo bash; fi To Join a network From the command line simply type zerotier-cli join ################ with ############### being the 16-digit network ID of the network you wish to join.","title":"How to install Zerotier"},{"location":"platforms/argo/","text":"Argo Tunnels This is a service from Cloudflare where you can set an outbound port and reach out to the Cloudflare CDN and create secure tunnels while not exposing your server to the web. Setup Cloudflared as a Service You have to make sure that /root/.cloudflared/config.yml is valid. Here is an example, hostname: <servername> credentials-file: /root/.cloudflared/<uuid for tunnel>.json tunnel: <tunnel name> ingress: # Rules map traffic from a hostname to a local service: - hostname: <yourdomain.com> service: https://localhost:<port listening on> - hostname: <anotherdoamin.com> service: https://<another server IP or port> When you run cloudflared service install it will pull the config and copy into /etc/cloudflared if its not valid then the systemd install won't work. Three files are generated in /etc/systemd/system which are, cloudflared.service cloudflared-update.service cloudflared-update.timer cloudflared.service [Unit] Description=Argo Tunnel After=network.target [Service] Ports Make sure 7844 and 443 are opened up. 7844 is used for the Argo traffic itself. 443 is used to update the cloudflared binary and for API calls to make sure the tunnels are up and connected.","title":"Argo"},{"location":"platforms/argo/#argo-tunnels","text":"This is a service from Cloudflare where you can set an outbound port and reach out to the Cloudflare CDN and create secure tunnels while not exposing your server to the web.","title":"Argo Tunnels"},{"location":"platforms/argo/#setup-cloudflared-as-a-service","text":"You have to make sure that /root/.cloudflared/config.yml is valid. Here is an example, hostname: <servername> credentials-file: /root/.cloudflared/<uuid for tunnel>.json tunnel: <tunnel name> ingress: # Rules map traffic from a hostname to a local service: - hostname: <yourdomain.com> service: https://localhost:<port listening on> - hostname: <anotherdoamin.com> service: https://<another server IP or port> When you run cloudflared service install it will pull the config and copy into /etc/cloudflared if its not valid then the systemd install won't work. Three files are generated in /etc/systemd/system which are, cloudflared.service cloudflared-update.service cloudflared-update.timer","title":"Setup Cloudflared as a Service"},{"location":"platforms/argo/#cloudflaredservice","text":"[Unit] Description=Argo Tunnel After=network.target [Service]","title":"cloudflared.service"},{"location":"platforms/argo/#ports","text":"Make sure 7844 and 443 are opened up. 7844 is used for the Argo traffic itself. 443 is used to update the cloudflared binary and for API calls to make sure the tunnels are up and connected.","title":"Ports"},{"location":"platforms/awscli/","text":"How to Update to a newer version curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip ./aws/install which aws ls -l /usr/local/bin/aws ./aws/install --bin-dir /usr/bin --install-dir /usr/bin/aws-cli --update If old version hangs around, look for the aws binary and then do the following and confirm the binary version find / -name aws /usr/bin/aws --version rm -f /usr/bin/aws ./aws/install --bin-dir /usr/bin --install-dir /usr/bin/aws-cli --update Job done so when you run aws --version it should return the correct version. Switch between AWS CLI Profiles To switch between profiles you can add --profile <profile name> Or just export AWS_PROFILE=<profile name> for multiple commands.","title":"AWS CLI"},{"location":"platforms/awscli/#how-to-update-to-a-newer-version","text":"curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip ./aws/install which aws ls -l /usr/local/bin/aws ./aws/install --bin-dir /usr/bin --install-dir /usr/bin/aws-cli --update If old version hangs around, look for the aws binary and then do the following and confirm the binary version find / -name aws /usr/bin/aws --version rm -f /usr/bin/aws ./aws/install --bin-dir /usr/bin --install-dir /usr/bin/aws-cli --update Job done so when you run aws --version it should return the correct version.","title":"How to Update to a newer version"},{"location":"platforms/awscli/#switch-between-aws-cli-profiles","text":"To switch between profiles you can add --profile <profile name> Or just export AWS_PROFILE=<profile name> for multiple commands.","title":"Switch between AWS CLI Profiles"},{"location":"platforms/awsgen/","text":"AWS General AWS Technical Essentials AWS Technical Essentials - S3 umlimited number of objects - no bucket size limit - HTTP/S enpoint to store and retrieve data - Optional server side encryption - Access logs for auditing - Stores data as objects within buckets - Flat hierarchy structure - eg https://doc.s3.amazonaws.com/2006-03-01/AmazonS3.html - Can upload and download via SSL - Bucket name must be globally unique - Objects are denied by default with the permissions Access permissions on S3 Define policy statements (there are examples in the policy generator) Amazon EBS Storage Same availability zone as EC2 instance When creating EBS volume Use cases OS - Use for boot/root vol, seocndary volumes Databases Enterprise apps Shared Responsibility - AWS (Security) Need to look into policy scripting for AWS, similiar for user access and general policy access IAM has no associated credentials IAM can be dynamically assigned Can use application authentication to be trusted in AWS environment to access particular resources? Database Services Push button deployment (RDS) EBS storage used Can take snapshots to different AWS regions Provides backups for DR Amazon DynamoDB No limit on data Quick performance provision and change the request capacity for each table Fully managed, NOSQL database service Over-ride VPC DNS DNS for EC2's in that VPC are determined by DHCP Sets, whcih define the default DNS any EC2's will use. On an ad-hoc basis with a few EC2's or when using IPSEC tunnel and want to use internal network DNS I can do the following on an EC2 to force it to use local DNS from internal LAN or any other DNS server I specify. Create or modifiy this file, /etc/dhcp/dhclient.conf Add the following so it looks like this, as an example. timeout 300; supersede domain-name-servers 10.130.22.21, 10.130.22.22; Modify /etc/sysconfig/network-scripts/ifcfg-eth0 Make sure the PEERDNS=yes is set. Reboot and good to go. More info on DNS for EC2 here, https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html","title":"AWS Gen"},{"location":"platforms/awsgen/#aws-general","text":"","title":"AWS General"},{"location":"platforms/awsgen/#aws-technical-essentials","text":"AWS Technical Essentials - S3 umlimited number of objects - no bucket size limit - HTTP/S enpoint to store and retrieve data - Optional server side encryption - Access logs for auditing - Stores data as objects within buckets - Flat hierarchy structure - eg https://doc.s3.amazonaws.com/2006-03-01/AmazonS3.html - Can upload and download via SSL - Bucket name must be globally unique - Objects are denied by default with the permissions","title":"AWS Technical Essentials"},{"location":"platforms/awsgen/#access-permissions-on-s3","text":"Define policy statements (there are examples in the policy generator)","title":"Access permissions on S3"},{"location":"platforms/awsgen/#amazon-ebs-storage","text":"Same availability zone as EC2 instance When creating EBS volume Use cases OS - Use for boot/root vol, seocndary volumes Databases Enterprise apps","title":"Amazon EBS Storage"},{"location":"platforms/awsgen/#shared-responsibility-aws-security","text":"Need to look into policy scripting for AWS, similiar for user access and general policy access IAM has no associated credentials IAM can be dynamically assigned Can use application authentication to be trusted in AWS environment to access particular resources?","title":"Shared Responsibility - AWS (Security)"},{"location":"platforms/awsgen/#database-services","text":"Push button deployment (RDS) EBS storage used Can take snapshots to different AWS regions Provides backups for DR","title":"Database Services"},{"location":"platforms/awsgen/#amazon-dynamodb","text":"No limit on data Quick performance provision and change the request capacity for each table Fully managed, NOSQL database service","title":"Amazon DynamoDB"},{"location":"platforms/awsgen/#over-ride-vpc-dns","text":"DNS for EC2's in that VPC are determined by DHCP Sets, whcih define the default DNS any EC2's will use. On an ad-hoc basis with a few EC2's or when using IPSEC tunnel and want to use internal network DNS I can do the following on an EC2 to force it to use local DNS from internal LAN or any other DNS server I specify. Create or modifiy this file, /etc/dhcp/dhclient.conf Add the following so it looks like this, as an example. timeout 300; supersede domain-name-servers 10.130.22.21, 10.130.22.22; Modify /etc/sysconfig/network-scripts/ifcfg-eth0 Make sure the PEERDNS=yes is set. Reboot and good to go. More info on DNS for EC2 here, https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html","title":"Over-ride VPC DNS"},{"location":"platforms/azure/","text":"Azure CLI The current version of the Azure CLI is 2.15.1 . For information about the latest release, see the release notes . To find your installed version and see if you need to update, run az version . Note The package for Azure CLI installs its own Python interpreter, and does not use the system Python. On Ubuntu 20.04 (Focal), there is an azure-cli package with version 2.0.81 provided by the focal/universe repository. It's outdated and not recommended. If you already installed it, please remove it first by running sudo apt remove azure-cli -y && sudo apt autoremove -y before following the below steps to install the latest azure-cli package. Install We offer two ways to install the Azure CLI with distributions that support apt : As an all-in-one script that runs the install commands for you, and instructions that you can run as a step-by-step process on your own. Install with one command We offer and maintain a script which runs all of the installation commands in one step. Run it by using curl and pipe directly to bash , or download the script to a file and inspect it before running. Important This script is only verified for Ubuntu 16.04+ and Debian 8+. It may not work on other distributions. If you're using a derived distribution such as Linux Mint, follow the manual install instructions and perform any necessary troubleshooting. curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash Manual install instructions If you don't want to run a script as superuser or the all-in-one script fails, follow these steps to install the Azure CLI. Get packages needed for the install process: ``` sudo apt-get update sudo apt-get install ca-certificates curl apt-transport-https lsb-release gnupg ``` 2. Download and install the Microsoft signing key: ``` curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg > /dev/null ``` 3. Add the Azure CLI software repository: ``` AZ_REPO=$(lsb_release -cs) echo \"deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main\" | sudo tee /etc/apt/sources.list.d/azure-cli.list `` 4. Update repository information and install the azure-cli` package: ``` sudo apt-get update sudo apt-get install azure-cli ``` Run the Azure CLI with the az command. To sign in, use the az login command. Run the login command. ``` az login ``` If the CLI can open your default browser, it will do so and load an Azure sign-in page. Otherwise, open a browser page at https://aka.ms/devicelogin and enter the authorization code displayed in your terminal. If no web browser is available or the web browser fails to open, use device code flow with az login --use-device-code . Sign in with your account credentials in the browser. To learn more about different authentication methods, see Sign in with Azure CLI . Troubleshooting Here are some common problems seen when installing with apt . If you experience a problem not covered here, file an issue on github . No module issue on Ubuntu 20.04 (Focal)/WSL If you installed azure-cli on Focal without adding the Azure CLI software repository in step 3 of the manual install instructions or using our script , you may encounter issues such as no module named 'decorator' or 'antlr4' as the package you installed is the outdated azure-cli 2.0.81 from the focal/universe repository. Please remove it first by running sudo apt remove azure-cli -y && sudo apt autoremove -y , then follow the above instructions to install the latest azure-cli package. lsb_release does not return the correct base distribution version Some Ubuntu- or Debian-derived distributions such as Linux Mint may not return the correct version name from lsb_release . This value is used in the install process to determine the package to install. If you know the code name of the Ubuntu or Debian version your distribution is derived from, you can set the AZ_REPO value manually when adding the repository . Otherwise, look up information for your distribution on how to determine the base distribution code name and set AZ_REPO to the correct value. No package for your distribution Sometimes it may be a while after a distribution is released before there's an Azure CLI package available for it. The Azure CLI designed to be resilient with regards to future versions of dependencies and rely on as few of them as possible. If there's no package available for your base distribution, try a package for an earlier distribution. To do this, set the value of AZ_REPO manually when adding the repository . For Ubuntu distributions use the bionic repository, and for Debian distributions use stretch . Distributions released before Ubuntu Trusty and Debian Wheezy are not supported. Elementary OS (EOS) fails to install the Azure CLI EOS fails to install the Azure cli because lsb_release returns HERA , which is the EOS release name. The solution is to fix the file /etc/apt/sources.list.d/azure-cli.list and change hera main to bionic main . Original file contents: deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ hera main Modified file contents deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ bionic main Proxy blocks connection If you're unable to connect to an external resource due to a proxy, make sure that you've correctly set the HTTP_PROXY and HTTPS_PROXY variables in your shell. You will need to contact your system administrator to know what host(s) and port(s) to use for these proxies. These values are respected by many Linux programs, including those which are used in the install process. To set these values: # No auth export HTTP_PROXY=http://[proxy]:[port] export HTTPS_PROXY=https://[proxy]:[port] # Basic auth export HTTP_PROXY=http://[username]:[password]@[proxy]:[port] export HTTPS_PROXY=https://[username]:[password]@[proxy]:[port] Important If you are behind a proxy, these shell variables must be set to connect to Azure services with the CLI. If you are not using basic auth, it's recommended to export these variables in your .bashrc file. Always follow your business' security policies and the requirements of your system administrator. You may also want to explicitly configure apt to use this proxy at all times. Make sure that the following lines appear in an apt configuration file in /etc/apt/apt.conf.d/ . We recommend using either your existing global configuration file, an existing proxy configuration file, 40proxies , or 99local , but follow your system administration requirements. Acquire { http::proxy \"http://[username]:[password]@[proxy]:[port]\"; https::proxy \"https://[username]:[password]@[proxy]:[port]\"; } If your proxy does not use basic auth, remove the [username]:[password]@ portion of the proxy URI. If you require more information for proxy configuration, see the official Ubuntu documentation: apt.conf manpage Ubuntu wiki - apt-get howto In order to get the Microsoft signing key and get the package from our repository, your proxy needs to allow HTTPS connections to the following address: https://packages.microsoft.com CLI fails to install or run on Windows Subsystem for Linux Since Windows Subsystem for Linux (WSL) is a system call translation layer on top of the Windows platform, you might experience an error when trying to install or run the Azure CLI. The CLI relies on some features that may have a bug in WSL. If you experience an error no matter how you install the CLI, there's a good chance it's an issue with WSL and not with the CLI install process. To troubleshoot your WSL installation and possibly resolve issues: If you can, run an identical install process on a Linux machine or VM to see if it succeeds. If it does, your issue is almost certainly related to WSL. To start a Linux VM in Azure, see the create a Linux VM in the Azure Portal documentation. Make sure that you're running the latest version of WSL. To get the latest version, update your Windows 10 installation . Check for any open issues with WSL which might address your problem. Often there will be suggestions on how to work around the problem, or information about a release where the issue will be fixed. If there are no existing issues for your problem, file a new issue with WSL and make sure that you include as much information as possible. If you continue to have issues installing or running on WSL, consider installing the CLI for Windows . Update The CLI provides an in-tool command to update to the latest version: az upgrade Note The az upgrade command was added in version 2.11.0 and will not work with versions prior to 2.11.0. This command will also update all installed extensions by default. For more az upgrade options, please refer to the command reference page . You can also use apt-get upgrade to update the CLI package. sudo apt-get update && sudo apt-get upgrade Note This command upgrades all of the installed packages on your system that have not had a dependency change. To upgrade the CLI only, use apt-get install . sudo apt-get update && sudo apt-get install --only-upgrade -y azure-cli Uninstall Uninstall with apt-get remove : ``` sudo apt-get remove -y azure-cli ``` 2. If you don't plan to reinstall the CLI, remove the Azure CLI repository information: ``` sudo rm /etc/apt/sources.list.d/azure-cli.list ``` 3. If you use no other packages from Microsoft, remove the signing key: ``` sudo rm /etc/apt/trusted.gpg.d/microsoft.gpg ``` 4. Remove any unneeded packages: ``` sudo apt autoremove ``` Create a Vault in Azure using CLI Make sure your logged into Azure first Have a storage acocunt created Check appropriate resource group az keyvault create --name \"enainfraterraformvault\" --resource-group \"ena-infra-terraform\" --location australiaeast az is azure cli command line keyvault create --name \"enainfraterraformvault\" is the name of keyvault to create --resource-group \"ena-infra-terraform\" is the name of resource group, this is very important. --location australiaeast` location of vault to be located. Reference Access key and export as environment variable This allows you to export Access key for Azure service which is stored in an Azure Vault without having the Access key saved to disk and increasing the security risk. Make sure your logged in to Azure by running az login from the cli and follow on screen instructions. export ARM_ACCESS_KEY=$(az keyvault secret show --name enainfraterraform --vault-name enainfraterraformvault --query value -o tsv) ARM_ACCESS_KEY is env variable name The rest refeferences the vault List all Locations in Azure in Table format This is how you list locations in Azure, there are a few output formats, like json , yaml as examples. az account list-locations --out table Output is something like this, DisplayName Name RegionalDisplayName ------------------------ ------------------- ------------------------------------- East US eastus (US) East US East US 2 eastus2 (US) East US 2 South Central US southcentralus (US) South Central US West US 2 westus2 (US) West US 2 Australia East australiaeast (Asia Pacific) Australia East Southeast Asia southeastasia (Asia Pacific) Southeast Asia North Europe northeurope (Europe) North Europe UK South uksouth (Europe) UK South West Europe westeurope (Europe) West Europe Central US centralus (US) Central US North Central US northcentralus (US) North Central US West US westus (US) West US South Africa North southafricanorth (Africa) South Africa North Central India centralindia (Asia Pacific) Central India East Asia eastasia (Asia Pacific) East Asia Japan East japaneast (Asia Pacific) Japan East Korea Central koreacentral (Asia Pacific) Korea Central Canada Central canadacentral (Canada) Canada Central France Central francecentral (Europe) France Central Germany West Central germanywestcentral (Europe) Germany West Central Norway East norwayeast (Europe) Norway East Switzerland North switzerlandnorth (Europe) Switzerland North UAE North uaenorth (Middle East) UAE North Brazil South brazilsouth (South America) Brazil South Central US (Stage) centralusstage (US) Central US (Stage) East US (Stage) eastusstage (US) East US (Stage) East US 2 (Stage) eastus2stage (US) East US 2 (Stage) North Central US (Stage) northcentralusstage (US) North Central US (Stage) South Central US (Stage) southcentralusstage (US) South Central US (Stage) West US (Stage) westusstage (US) West US (Stage) West US 2 (Stage) westus2stage (US) West US 2 (Stage) Asia asia Asia Asia Pacific asiapacific Asia Pacific Australia australia Australia Brazil brazil Brazil Canada canada Canada Europe europe Europe Global global Global India india India Japan japan Japan United Kingdom uk United Kingdom United States unitedstates United States East Asia (Stage) eastasiastage (Asia Pacific) East Asia (Stage) Southeast Asia (Stage) southeastasiastage (Asia Pacific) Southeast Asia (Stage) East US 2 EUAP eastus2euap (US) East US 2 EUAP West Central US westcentralus (US) West Central US South Africa West southafricawest (Africa) South Africa West Australia Central australiacentral (Asia Pacific) Australia Central Australia Central 2 australiacentral2 (Asia Pacific) Australia Central 2 Australia Southeast australiasoutheast (Asia Pacific) Australia Southeast Japan West japanwest (Asia Pacific) Japan West Korea South koreasouth (Asia Pacific) Korea South South India southindia (Asia Pacific) South India West India westindia (Asia Pacific) West India Canada East canadaeast (Canada) Canada East France South francesouth (Europe) France South Germany North germanynorth (Europe) Germany North Norway West norwaywest (Europe) Norway West Switzerland West switzerlandwest (Europe) Switzerland West UK West ukwest (Europe) UK West UAE Central uaecentral (Middle East) UAE Central Brazil Southeast brazilsoutheast (South America) Brazil Southeast Azure Storage using CLI Upload a file to blob container in Azure This command will allow you to upload files to a specific blob container in Azure az storage blob upload --account-name enainfraterraform --container-name tfstate --name helloworld-test --file helloworld-test --auth-mode key --account-key $ARM_ACCESS_KEY az storage blob upload --account-name storage account to upload to --container-name the blob you want to upload into --name name to call when uploaded --file the actual filename --auth-mode key you need to setup an env variable with access key first to use this option --account-key $ARM_ACCESS_KEY name of env variable you've used to store the access key in If successful you will get some like this, Finished[#############################################################] 100.0000% { \"etag\": \"\\\"0x8D85906BA9BD4E6\\\"\", \"lastModified\": \"2020-09-14T23:34:30+00:00\" } Download a file from a blob container or collection of files To download files recursively, here is how you do it. az storage blob download-batch --account-name enainfraterraform -d . -s tfstate --pattern test/* --auth-mode key --account-key $ARM_ACCESS_KEY Quickstart - Set and retreive a secret from Azure Key Vault Prerequisites Use Azure Cloud Shell using the bash environment. If you prefer, install Azure CLI to run CLI reference commands. If you're using a local install, sign in with Azure CLI by using the az login command. To finish the authentication process, follow the steps displayed in your terminal. See Sign in with Azure CLI for additional sign-in options. When you're prompted, install Azure CLI extensions on first use. For more information about extensions, see Use extensions with Azure CLI . Run az version to find the version and dependent libraries that are installed. To upgrade to the latest version, run az upgrade . This quickstart requires version 2.0.4 or later of the Azure CLI. If using Azure Cloud Shell, the latest version is already installed. Create a resource group A resource group is a logical container into which Azure resources are deployed and managed. The following example creates a resource group named ContosoResourceGroup in the eastus location. Azure CLI az group create --name \"ContosoResourceGroup\" --location eastus Create a Key Vault Next you will create a Key Vault in the resource group created in the previous step. You will need to provide some information: For this quickstart we use Contoso-vault2 . You must provide a unique name in your testing. Resource group name ContosoResourceGroup . The location East US . Azure CLI az keyvault create --name \"Contoso-Vault2\" --resource-group \"ContosoResourceGroup\" --location eastus The output of this cmdlet shows properties of the newly created Key Vault. Take note of the two properties listed below: Vault Name : In the example, this is Contoso-Vault2 . You will use this name for other Key Vault commands. Vault URI : In the example, this is https://contoso-vault2.vault.azure.net/ . Applications that use your vault through its REST API must use this URI. At this point, your Azure account is the only one authorized to perform any operations on this new vault. Add a secret to Key Vault To add a secret to the vault, you just need to take a couple of additional steps. This password could be used by an application. The password will be called ExamplePassword and will store the value of hVFkk965BuUv in it. Type the commands below to create a secret in Key Vault called ExamplePassword that will store the value hVFkk965BuUv : Azure CLI az keyvault secret set --vault-name \"Contoso-Vault2\" --name \"ExamplePassword\" --value \"hVFkk965BuUv\" You can now reference this password that you added to Azure Key Vault by using its URI. Use 'https://Contoso-Vault2.vault.azure.net/secrets/ExamplePassword' to get the current version. To view the value contained in the secret as plain text: Azure CLI az keyvault secret show --name \"ExamplePassword\" --vault-name \"Contoso-Vault2\" Now, you have created a Key Vault, stored a secret, and retrieved it. Clean up resources Other quickstarts and tutorials in this collection build upon this quickstart. If you plan to continue on to work with subsequent quickstarts and tutorials, you may wish to leave these resources in place. When no longer needed, you can use the az group delete command to remove the resource group, and all related resources. You can delete the resources as follows: Azure CLI az group delete --name ContosoResourceGroup","title":"Azure CLI"},{"location":"platforms/azure/#azure-cli","text":"The current version of the Azure CLI is 2.15.1 . For information about the latest release, see the release notes . To find your installed version and see if you need to update, run az version . Note The package for Azure CLI installs its own Python interpreter, and does not use the system Python. On Ubuntu 20.04 (Focal), there is an azure-cli package with version 2.0.81 provided by the focal/universe repository. It's outdated and not recommended. If you already installed it, please remove it first by running sudo apt remove azure-cli -y && sudo apt autoremove -y before following the below steps to install the latest azure-cli package.","title":"Azure CLI"},{"location":"platforms/azure/#install","text":"We offer two ways to install the Azure CLI with distributions that support apt : As an all-in-one script that runs the install commands for you, and instructions that you can run as a step-by-step process on your own.","title":"Install"},{"location":"platforms/azure/#install-with-one-command","text":"We offer and maintain a script which runs all of the installation commands in one step. Run it by using curl and pipe directly to bash , or download the script to a file and inspect it before running. Important This script is only verified for Ubuntu 16.04+ and Debian 8+. It may not work on other distributions. If you're using a derived distribution such as Linux Mint, follow the manual install instructions and perform any necessary troubleshooting. curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash","title":"Install with one command"},{"location":"platforms/azure/#manual-install-instructions","text":"If you don't want to run a script as superuser or the all-in-one script fails, follow these steps to install the Azure CLI. Get packages needed for the install process: ``` sudo apt-get update sudo apt-get install ca-certificates curl apt-transport-https lsb-release gnupg ``` 2. Download and install the Microsoft signing key: ``` curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg > /dev/null ``` 3. Add the Azure CLI software repository: ``` AZ_REPO=$(lsb_release -cs) echo \"deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main\" | sudo tee /etc/apt/sources.list.d/azure-cli.list `` 4. Update repository information and install the azure-cli` package: ``` sudo apt-get update sudo apt-get install azure-cli ``` Run the Azure CLI with the az command. To sign in, use the az login command. Run the login command. ``` az login ``` If the CLI can open your default browser, it will do so and load an Azure sign-in page. Otherwise, open a browser page at https://aka.ms/devicelogin and enter the authorization code displayed in your terminal. If no web browser is available or the web browser fails to open, use device code flow with az login --use-device-code . Sign in with your account credentials in the browser. To learn more about different authentication methods, see Sign in with Azure CLI .","title":"Manual install instructions"},{"location":"platforms/azure/#troubleshooting","text":"Here are some common problems seen when installing with apt . If you experience a problem not covered here, file an issue on github .","title":"Troubleshooting"},{"location":"platforms/azure/#no-module-issue-on-ubuntu-2004-focalwsl","text":"If you installed azure-cli on Focal without adding the Azure CLI software repository in step 3 of the manual install instructions or using our script , you may encounter issues such as no module named 'decorator' or 'antlr4' as the package you installed is the outdated azure-cli 2.0.81 from the focal/universe repository. Please remove it first by running sudo apt remove azure-cli -y && sudo apt autoremove -y , then follow the above instructions to install the latest azure-cli package.","title":"No module issue on Ubuntu 20.04 (Focal)/WSL"},{"location":"platforms/azure/#lsb_release-does-not-return-the-correct-base-distribution-version","text":"Some Ubuntu- or Debian-derived distributions such as Linux Mint may not return the correct version name from lsb_release . This value is used in the install process to determine the package to install. If you know the code name of the Ubuntu or Debian version your distribution is derived from, you can set the AZ_REPO value manually when adding the repository . Otherwise, look up information for your distribution on how to determine the base distribution code name and set AZ_REPO to the correct value.","title":"lsb_release does not return the correct base distribution version"},{"location":"platforms/azure/#no-package-for-your-distribution","text":"Sometimes it may be a while after a distribution is released before there's an Azure CLI package available for it. The Azure CLI designed to be resilient with regards to future versions of dependencies and rely on as few of them as possible. If there's no package available for your base distribution, try a package for an earlier distribution. To do this, set the value of AZ_REPO manually when adding the repository . For Ubuntu distributions use the bionic repository, and for Debian distributions use stretch . Distributions released before Ubuntu Trusty and Debian Wheezy are not supported.","title":"No package for your distribution"},{"location":"platforms/azure/#elementary-os-eos-fails-to-install-the-azure-cli","text":"EOS fails to install the Azure cli because lsb_release returns HERA , which is the EOS release name. The solution is to fix the file /etc/apt/sources.list.d/azure-cli.list and change hera main to bionic main . Original file contents: deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ hera main Modified file contents deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ bionic main","title":"Elementary OS (EOS) fails to install the Azure CLI"},{"location":"platforms/azure/#proxy-blocks-connection","text":"If you're unable to connect to an external resource due to a proxy, make sure that you've correctly set the HTTP_PROXY and HTTPS_PROXY variables in your shell. You will need to contact your system administrator to know what host(s) and port(s) to use for these proxies. These values are respected by many Linux programs, including those which are used in the install process. To set these values: # No auth export HTTP_PROXY=http://[proxy]:[port] export HTTPS_PROXY=https://[proxy]:[port] # Basic auth export HTTP_PROXY=http://[username]:[password]@[proxy]:[port] export HTTPS_PROXY=https://[username]:[password]@[proxy]:[port] Important If you are behind a proxy, these shell variables must be set to connect to Azure services with the CLI. If you are not using basic auth, it's recommended to export these variables in your .bashrc file. Always follow your business' security policies and the requirements of your system administrator. You may also want to explicitly configure apt to use this proxy at all times. Make sure that the following lines appear in an apt configuration file in /etc/apt/apt.conf.d/ . We recommend using either your existing global configuration file, an existing proxy configuration file, 40proxies , or 99local , but follow your system administration requirements. Acquire { http::proxy \"http://[username]:[password]@[proxy]:[port]\"; https::proxy \"https://[username]:[password]@[proxy]:[port]\"; } If your proxy does not use basic auth, remove the [username]:[password]@ portion of the proxy URI. If you require more information for proxy configuration, see the official Ubuntu documentation: apt.conf manpage Ubuntu wiki - apt-get howto In order to get the Microsoft signing key and get the package from our repository, your proxy needs to allow HTTPS connections to the following address: https://packages.microsoft.com","title":"Proxy blocks connection"},{"location":"platforms/azure/#cli-fails-to-install-or-run-on-windows-subsystem-for-linux","text":"Since Windows Subsystem for Linux (WSL) is a system call translation layer on top of the Windows platform, you might experience an error when trying to install or run the Azure CLI. The CLI relies on some features that may have a bug in WSL. If you experience an error no matter how you install the CLI, there's a good chance it's an issue with WSL and not with the CLI install process. To troubleshoot your WSL installation and possibly resolve issues: If you can, run an identical install process on a Linux machine or VM to see if it succeeds. If it does, your issue is almost certainly related to WSL. To start a Linux VM in Azure, see the create a Linux VM in the Azure Portal documentation. Make sure that you're running the latest version of WSL. To get the latest version, update your Windows 10 installation . Check for any open issues with WSL which might address your problem. Often there will be suggestions on how to work around the problem, or information about a release where the issue will be fixed. If there are no existing issues for your problem, file a new issue with WSL and make sure that you include as much information as possible. If you continue to have issues installing or running on WSL, consider installing the CLI for Windows .","title":"CLI fails to install or run on Windows Subsystem for Linux"},{"location":"platforms/azure/#update","text":"The CLI provides an in-tool command to update to the latest version: az upgrade Note The az upgrade command was added in version 2.11.0 and will not work with versions prior to 2.11.0. This command will also update all installed extensions by default. For more az upgrade options, please refer to the command reference page . You can also use apt-get upgrade to update the CLI package. sudo apt-get update && sudo apt-get upgrade Note This command upgrades all of the installed packages on your system that have not had a dependency change. To upgrade the CLI only, use apt-get install . sudo apt-get update && sudo apt-get install --only-upgrade -y azure-cli","title":"Update"},{"location":"platforms/azure/#uninstall","text":"Uninstall with apt-get remove : ``` sudo apt-get remove -y azure-cli ``` 2. If you don't plan to reinstall the CLI, remove the Azure CLI repository information: ``` sudo rm /etc/apt/sources.list.d/azure-cli.list ``` 3. If you use no other packages from Microsoft, remove the signing key: ``` sudo rm /etc/apt/trusted.gpg.d/microsoft.gpg ``` 4. Remove any unneeded packages: ``` sudo apt autoremove ```","title":"Uninstall"},{"location":"platforms/azure/#create-a-vault-in-azure-using-cli","text":"Make sure your logged into Azure first Have a storage acocunt created Check appropriate resource group az keyvault create --name \"enainfraterraformvault\" --resource-group \"ena-infra-terraform\" --location australiaeast az is azure cli command line keyvault create --name \"enainfraterraformvault\" is the name of keyvault to create --resource-group \"ena-infra-terraform\" is the name of resource group, this is very important. --location australiaeast` location of vault to be located.","title":"Create a Vault in Azure using CLI"},{"location":"platforms/azure/#reference-access-key-and-export-as-environment-variable","text":"This allows you to export Access key for Azure service which is stored in an Azure Vault without having the Access key saved to disk and increasing the security risk. Make sure your logged in to Azure by running az login from the cli and follow on screen instructions. export ARM_ACCESS_KEY=$(az keyvault secret show --name enainfraterraform --vault-name enainfraterraformvault --query value -o tsv) ARM_ACCESS_KEY is env variable name The rest refeferences the vault","title":"Reference Access key and export as environment variable"},{"location":"platforms/azure/#list-all-locations-in-azure-in-table-format","text":"This is how you list locations in Azure, there are a few output formats, like json , yaml as examples. az account list-locations --out table Output is something like this, DisplayName Name RegionalDisplayName ------------------------ ------------------- ------------------------------------- East US eastus (US) East US East US 2 eastus2 (US) East US 2 South Central US southcentralus (US) South Central US West US 2 westus2 (US) West US 2 Australia East australiaeast (Asia Pacific) Australia East Southeast Asia southeastasia (Asia Pacific) Southeast Asia North Europe northeurope (Europe) North Europe UK South uksouth (Europe) UK South West Europe westeurope (Europe) West Europe Central US centralus (US) Central US North Central US northcentralus (US) North Central US West US westus (US) West US South Africa North southafricanorth (Africa) South Africa North Central India centralindia (Asia Pacific) Central India East Asia eastasia (Asia Pacific) East Asia Japan East japaneast (Asia Pacific) Japan East Korea Central koreacentral (Asia Pacific) Korea Central Canada Central canadacentral (Canada) Canada Central France Central francecentral (Europe) France Central Germany West Central germanywestcentral (Europe) Germany West Central Norway East norwayeast (Europe) Norway East Switzerland North switzerlandnorth (Europe) Switzerland North UAE North uaenorth (Middle East) UAE North Brazil South brazilsouth (South America) Brazil South Central US (Stage) centralusstage (US) Central US (Stage) East US (Stage) eastusstage (US) East US (Stage) East US 2 (Stage) eastus2stage (US) East US 2 (Stage) North Central US (Stage) northcentralusstage (US) North Central US (Stage) South Central US (Stage) southcentralusstage (US) South Central US (Stage) West US (Stage) westusstage (US) West US (Stage) West US 2 (Stage) westus2stage (US) West US 2 (Stage) Asia asia Asia Asia Pacific asiapacific Asia Pacific Australia australia Australia Brazil brazil Brazil Canada canada Canada Europe europe Europe Global global Global India india India Japan japan Japan United Kingdom uk United Kingdom United States unitedstates United States East Asia (Stage) eastasiastage (Asia Pacific) East Asia (Stage) Southeast Asia (Stage) southeastasiastage (Asia Pacific) Southeast Asia (Stage) East US 2 EUAP eastus2euap (US) East US 2 EUAP West Central US westcentralus (US) West Central US South Africa West southafricawest (Africa) South Africa West Australia Central australiacentral (Asia Pacific) Australia Central Australia Central 2 australiacentral2 (Asia Pacific) Australia Central 2 Australia Southeast australiasoutheast (Asia Pacific) Australia Southeast Japan West japanwest (Asia Pacific) Japan West Korea South koreasouth (Asia Pacific) Korea South South India southindia (Asia Pacific) South India West India westindia (Asia Pacific) West India Canada East canadaeast (Canada) Canada East France South francesouth (Europe) France South Germany North germanynorth (Europe) Germany North Norway West norwaywest (Europe) Norway West Switzerland West switzerlandwest (Europe) Switzerland West UK West ukwest (Europe) UK West UAE Central uaecentral (Middle East) UAE Central Brazil Southeast brazilsoutheast (South America) Brazil Southeast","title":"List all Locations in Azure in Table format"},{"location":"platforms/azure/#azure-storage-using-cli","text":"","title":"Azure Storage using CLI"},{"location":"platforms/azure/#upload-a-file-to-blob-container-in-azure","text":"This command will allow you to upload files to a specific blob container in Azure az storage blob upload --account-name enainfraterraform --container-name tfstate --name helloworld-test --file helloworld-test --auth-mode key --account-key $ARM_ACCESS_KEY az storage blob upload --account-name storage account to upload to --container-name the blob you want to upload into --name name to call when uploaded --file the actual filename --auth-mode key you need to setup an env variable with access key first to use this option --account-key $ARM_ACCESS_KEY name of env variable you've used to store the access key in If successful you will get some like this, Finished[#############################################################] 100.0000% { \"etag\": \"\\\"0x8D85906BA9BD4E6\\\"\", \"lastModified\": \"2020-09-14T23:34:30+00:00\" }","title":"Upload a file to blob container in Azure"},{"location":"platforms/azure/#download-a-file-from-a-blob-container-or-collection-of-files","text":"To download files recursively, here is how you do it. az storage blob download-batch --account-name enainfraterraform -d . -s tfstate --pattern test/* --auth-mode key --account-key $ARM_ACCESS_KEY","title":"Download a file from a blob container or collection of files"},{"location":"platforms/azure/#quickstart-set-and-retreive-a-secret-from-azure-key-vault","text":"","title":"Quickstart - Set and retreive a secret from Azure Key Vault"},{"location":"platforms/azure/#prerequisites","text":"Use Azure Cloud Shell using the bash environment. If you prefer, install Azure CLI to run CLI reference commands. If you're using a local install, sign in with Azure CLI by using the az login command. To finish the authentication process, follow the steps displayed in your terminal. See Sign in with Azure CLI for additional sign-in options. When you're prompted, install Azure CLI extensions on first use. For more information about extensions, see Use extensions with Azure CLI . Run az version to find the version and dependent libraries that are installed. To upgrade to the latest version, run az upgrade . This quickstart requires version 2.0.4 or later of the Azure CLI. If using Azure Cloud Shell, the latest version is already installed.","title":"Prerequisites"},{"location":"platforms/azure/#create-a-resource-group","text":"A resource group is a logical container into which Azure resources are deployed and managed. The following example creates a resource group named ContosoResourceGroup in the eastus location. Azure CLI az group create --name \"ContosoResourceGroup\" --location eastus","title":"Create a resource group"},{"location":"platforms/azure/#create-a-key-vault","text":"Next you will create a Key Vault in the resource group created in the previous step. You will need to provide some information: For this quickstart we use Contoso-vault2 . You must provide a unique name in your testing. Resource group name ContosoResourceGroup . The location East US . Azure CLI az keyvault create --name \"Contoso-Vault2\" --resource-group \"ContosoResourceGroup\" --location eastus The output of this cmdlet shows properties of the newly created Key Vault. Take note of the two properties listed below: Vault Name : In the example, this is Contoso-Vault2 . You will use this name for other Key Vault commands. Vault URI : In the example, this is https://contoso-vault2.vault.azure.net/ . Applications that use your vault through its REST API must use this URI. At this point, your Azure account is the only one authorized to perform any operations on this new vault.","title":"Create a Key Vault"},{"location":"platforms/azure/#add-a-secret-to-key-vault","text":"To add a secret to the vault, you just need to take a couple of additional steps. This password could be used by an application. The password will be called ExamplePassword and will store the value of hVFkk965BuUv in it. Type the commands below to create a secret in Key Vault called ExamplePassword that will store the value hVFkk965BuUv : Azure CLI az keyvault secret set --vault-name \"Contoso-Vault2\" --name \"ExamplePassword\" --value \"hVFkk965BuUv\" You can now reference this password that you added to Azure Key Vault by using its URI. Use 'https://Contoso-Vault2.vault.azure.net/secrets/ExamplePassword' to get the current version. To view the value contained in the secret as plain text: Azure CLI az keyvault secret show --name \"ExamplePassword\" --vault-name \"Contoso-Vault2\" Now, you have created a Key Vault, stored a secret, and retrieved it.","title":"Add a secret to Key Vault"},{"location":"platforms/azure/#clean-up-resources","text":"Other quickstarts and tutorials in this collection build upon this quickstart. If you plan to continue on to work with subsequent quickstarts and tutorials, you may wish to leave these resources in place. When no longer needed, you can use the az group delete command to remove the resource group, and all related resources. You can delete the resources as follows: Azure CLI az group delete --name ContosoResourceGroup","title":"Clean up resources"},{"location":"platforms/ciscoepnmn/","text":"Cico EPNMN CLI Commands Managing NCS Services Get status of main NCS services ncs status On reboot, sometimes these services don't startup. So run the following. ncs start Perform an NCS Cleanup This cleans up logs files, old backup files etc, good for freeing up disk space. In exec mode, ncs cleanup Managing Network interfaces Remove interface from HA in EXEC mode ncs ha monitor interface del GigabitEthernet 0 Change IP Address In EXEC mode show interface to enter config mode configure To configure interface and change IP interface GigabitEthernet 0 This will put youn into the interface sub-menu. ip address <ip address> <subnet> You'll be asked to restart interface. Then exit out of configure and savin g config by doing the following, write memory Then check it by running the following, show startup-config Change time zone Display time zone in EXEC mode, show timezone Then enter config mode and change by doing this, configure clock timezone <timezone> exit then save config write memory","title":"EPNMN CLI"},{"location":"platforms/ciscoepnmn/#cico-epnmn-cli-commands","text":"","title":"Cico EPNMN CLI Commands"},{"location":"platforms/ciscoepnmn/#managing-ncs-services","text":"Get status of main NCS services ncs status On reboot, sometimes these services don't startup. So run the following. ncs start","title":"Managing NCS Services"},{"location":"platforms/ciscoepnmn/#perform-an-ncs-cleanup","text":"This cleans up logs files, old backup files etc, good for freeing up disk space. In exec mode, ncs cleanup","title":"Perform an NCS Cleanup"},{"location":"platforms/ciscoepnmn/#managing-network-interfaces","text":"","title":"Managing Network interfaces"},{"location":"platforms/ciscoepnmn/#remove-interface-from-ha","text":"in EXEC mode ncs ha monitor interface del GigabitEthernet 0","title":"Remove interface from HA"},{"location":"platforms/ciscoepnmn/#change-ip-address","text":"In EXEC mode show interface to enter config mode configure To configure interface and change IP interface GigabitEthernet 0 This will put youn into the interface sub-menu. ip address <ip address> <subnet> You'll be asked to restart interface. Then exit out of configure and savin g config by doing the following, write memory Then check it by running the following, show startup-config","title":"Change IP Address"},{"location":"platforms/ciscoepnmn/#change-time-zone","text":"Display time zone in EXEC mode, show timezone Then enter config mode and change by doing this, configure clock timezone <timezone> exit then save config write memory","title":"Change time zone"},{"location":"platforms/proxmox/","text":"Proxmox ProxMox Initial setup Disable Commercial Repo sed -i \"s/^deb/\\#deb/\" /etc/apt/sources.list.d/pve-enterprise.list apt-get update Add PVE Community Repo echo \"deb http://download.proxmox.com/debian/pve $(grep \"VERSION=\" /etc/os-release | sed -n 's/.*(\\(.*\\)).*/\\1/p') pve-no-subscription\" > /etc/apt/sources.list.d/pve-no-enterprise.list apt-get update Remove nag echo \"DPkg::Post-Invoke { \\\"dpkg -V proxmox-widget-toolkit | grep -q '/proxmoxlib\\.js$'; if [ \\$? -eq 1 ]; then { echo 'Removing subscription nag from UI...'; sed -i '/data.status/{s/\\!//;s/Active/NoMoreNagging/}' /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js; }; fi\\\"; };\" > /etc/apt/apt.conf.d/no-nag-script apt --reinstall install proxmox-widget-toolkit Install qemu agent apt-get install -y qemu-guest-agent systemctl enable qemu-guest-agent systemctl start qemu-guest-agent LXC Containers on Proxmox LXC containers are containers much like Docker but unlike docker have their own namespaces and utilise the host system kernel. I use LXC for a variety of purposes. Mainly for services I want to isolate and have several dependencies around it. For example, Pi-Hole or a reverse proxy. These can be run from Docker as well but I like the fact I can manage these LXC containers through Proxmox UI. Anyway, some caveats I found when creating LXC containers. SSH is not enabled by default space on filesystem to create SSH session doesn't work. So heres what you do. systemctl start sshd mkdir /run/sshd Thats it. Proxmox further setup Add a custom backup location Located in `/etc/pve/storage.cfg' dir: backup path /mnt/backup content backup maxfiles 7 Install Samba for Windows Shares Do the following to add a smba share and have R/W access. apt install samba -y adduser <username> smdpasswd -d <username> smbdpasswd -a <username> nano /etc/samba/smb.conf Add the following, [<share label>] comment = <description of share> path = <location you want to share> browseable = yes read only = no writable = yes write list = <username> valid users = <username> guest = no create mask = 0775 directory mask = 0775 systemctl enable smbd systemctl restart smbd Run Proxmox from a Laptop I run proxmox on an old laptop for several reasons.~ Repurposing old hardware.~ Its relatively quiet.~ Low power consumption.~ Built in UPS (battery)~ Small foot print.~ Once installed there a few extra things to consider. Disable suspension on lid closure Don't want laptop to shutdown when the lid is closed, Run this, systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target Change AppArmour policy for NFS mounts in CT's nano /etc/apparmor.d/lxc/lxc-default-cgns Change it to this, # Do not load this file. Rather, load /etc/apparmor.d/lxc-containers, which # will source all profiles under /etc/apparmor.d/lxc profile lxc-container-default-cgns flags=(attach_disconnected,mediate_deleted) { #include <abstractions/lxc/container-base> # the container may never be allowed to mount devpts. If it does, it # will remount the host's devpts. We could allow it to do it with # the newinstance option (but, right now, we don't). deny mount fstype=devpts, # Extra stuff added mount fstype=nfs, mount fstype=cgroup -> /sys/fs/cgroup/**, mount fstype=cgroup2 -> /sys/fs/cgroup/**, } Then reload apparmour service, service apparmor reload Add NFS Mount inside a Proxmox Container Make sure the container is down. Run the following in the main proxmox console pct set 1000 -mp0 /mnt/pve/data,mp=/mnt/data Lets break it down, pct set 1000 is the container to apply it to. -mp0 the mount point, if you wanted multiple you would add -mp1 as an example. /mnt/pve/data is the local mount point in proxmox not in the container. mp=/mnt/data is the mount point created within the container when it starts up. Note you don't need to add anything in the /etc/fstab config in the container. I tried this approach and could never get it working the way I wanted. Kill a Container or VM that won't shutdown Log into shell for proxmox host. ps aux | grep VMID You'll get the pid that way then, kill -9 pid","title":"Proxmox"},{"location":"platforms/proxmox/#proxmox","text":"","title":"Proxmox"},{"location":"platforms/proxmox/#proxmox-initial-setup","text":"","title":"ProxMox Initial setup"},{"location":"platforms/proxmox/#disable-commercial-repo","text":"sed -i \"s/^deb/\\#deb/\" /etc/apt/sources.list.d/pve-enterprise.list apt-get update","title":"Disable Commercial Repo"},{"location":"platforms/proxmox/#add-pve-community-repo","text":"echo \"deb http://download.proxmox.com/debian/pve $(grep \"VERSION=\" /etc/os-release | sed -n 's/.*(\\(.*\\)).*/\\1/p') pve-no-subscription\" > /etc/apt/sources.list.d/pve-no-enterprise.list apt-get update","title":"Add PVE Community Repo"},{"location":"platforms/proxmox/#remove-nag","text":"echo \"DPkg::Post-Invoke { \\\"dpkg -V proxmox-widget-toolkit | grep -q '/proxmoxlib\\.js$'; if [ \\$? -eq 1 ]; then { echo 'Removing subscription nag from UI...'; sed -i '/data.status/{s/\\!//;s/Active/NoMoreNagging/}' /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js; }; fi\\\"; };\" > /etc/apt/apt.conf.d/no-nag-script apt --reinstall install proxmox-widget-toolkit","title":"Remove nag"},{"location":"platforms/proxmox/#install-qemu-agent","text":"apt-get install -y qemu-guest-agent systemctl enable qemu-guest-agent systemctl start qemu-guest-agent","title":"Install qemu agent"},{"location":"platforms/proxmox/#lxc-containers-on-proxmox","text":"LXC containers are containers much like Docker but unlike docker have their own namespaces and utilise the host system kernel. I use LXC for a variety of purposes. Mainly for services I want to isolate and have several dependencies around it. For example, Pi-Hole or a reverse proxy. These can be run from Docker as well but I like the fact I can manage these LXC containers through Proxmox UI. Anyway, some caveats I found when creating LXC containers. SSH is not enabled by default space on filesystem to create SSH session doesn't work. So heres what you do. systemctl start sshd mkdir /run/sshd Thats it.","title":"LXC Containers on Proxmox"},{"location":"platforms/proxmox/#proxmox-further-setup","text":"","title":"Proxmox further setup"},{"location":"platforms/proxmox/#add-a-custom-backup-location","text":"Located in `/etc/pve/storage.cfg' dir: backup path /mnt/backup content backup maxfiles 7","title":"Add a custom backup location"},{"location":"platforms/proxmox/#install-samba-for-windows-shares","text":"Do the following to add a smba share and have R/W access. apt install samba -y adduser <username> smdpasswd -d <username> smbdpasswd -a <username> nano /etc/samba/smb.conf Add the following, [<share label>] comment = <description of share> path = <location you want to share> browseable = yes read only = no writable = yes write list = <username> valid users = <username> guest = no create mask = 0775 directory mask = 0775 systemctl enable smbd systemctl restart smbd","title":"Install Samba for Windows Shares"},{"location":"platforms/proxmox/#run-proxmox-from-a-laptop","text":"I run proxmox on an old laptop for several reasons.~ Repurposing old hardware.~ Its relatively quiet.~ Low power consumption.~ Built in UPS (battery)~ Small foot print.~ Once installed there a few extra things to consider.","title":"Run Proxmox from a Laptop"},{"location":"platforms/proxmox/#disable-suspension-on-lid-closure","text":"Don't want laptop to shutdown when the lid is closed, Run this, systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target","title":"Disable suspension on lid closure"},{"location":"platforms/proxmox/#change-apparmour-policy-for-nfs-mounts-in-cts","text":"nano /etc/apparmor.d/lxc/lxc-default-cgns Change it to this, # Do not load this file. Rather, load /etc/apparmor.d/lxc-containers, which # will source all profiles under /etc/apparmor.d/lxc profile lxc-container-default-cgns flags=(attach_disconnected,mediate_deleted) { #include <abstractions/lxc/container-base> # the container may never be allowed to mount devpts. If it does, it # will remount the host's devpts. We could allow it to do it with # the newinstance option (but, right now, we don't). deny mount fstype=devpts, # Extra stuff added mount fstype=nfs, mount fstype=cgroup -> /sys/fs/cgroup/**, mount fstype=cgroup2 -> /sys/fs/cgroup/**, } Then reload apparmour service, service apparmor reload","title":"Change AppArmour policy for NFS mounts in CT's"},{"location":"platforms/proxmox/#add-nfs-mount-inside-a-proxmox-container","text":"Make sure the container is down. Run the following in the main proxmox console pct set 1000 -mp0 /mnt/pve/data,mp=/mnt/data Lets break it down, pct set 1000 is the container to apply it to. -mp0 the mount point, if you wanted multiple you would add -mp1 as an example. /mnt/pve/data is the local mount point in proxmox not in the container. mp=/mnt/data is the mount point created within the container when it starts up. Note you don't need to add anything in the /etc/fstab config in the container. I tried this approach and could never get it working the way I wanted.","title":"Add NFS Mount inside a Proxmox Container"},{"location":"platforms/proxmox/#kill-a-container-or-vm-that-wont-shutdown","text":"Log into shell for proxmox host. ps aux | grep VMID You'll get the pid that way then, kill -9 pid","title":"Kill a Container or VM that won't shutdown"},{"location":"platforms/vmware/","text":"Most common CLI commands for VMWare ESXi. Use SSH or esxi shells. vim-cmd vmsvc/getallvms List all VMs running on the host. Also provides vmid, required for commands below. vim-cmd vmsvc/power.off vmid Power off specified VM. vim-cmd vmsvc/power.on vmid Power off specified VM. vim-cmd vmsvc/power.reboot vmid Reboot specified VM. vim-cmd solo/registervm /vmfs/volume/datastore/subdir/vm-file.vmx Register the VM stored at location on the ESX host inventory. vim-cmd vmsvc/unregister vmid Unregister VM from the host. Does not remove the VM's files from the datastore. vim-cmd vmsvc/destroy vmid Delete the specified VM. The VMDK and VMX files will be deleted from storage as well. vim-cmd vmsvc/tools.install vmid Initiates an installation of VMWare Tools on the VM vim-cmd hostsvc/maintenance_mode_enter Put the host into maintenance mode. vim-cmd hostsvc/maintenance_mode_exit Take the host out of maintenance mode. vim-cmd hostsvc/net/info Show networking information of the host. chkconfig -l Show services running on the host. Can also be used to change startup configuration. esxtop Display list of processes and its usage of resources. Works similar to linux top. esxcfg-info Show host's configuration and information. esxcfg-nics -l Show current NIC configuration. esxcfg-vswitch -l Show current vSwitch configuration. vmkerrcode -l Display a reference list of VMKernel return codes and descriptions. dcui Start the console UI (when accessing through SSH). vsish Run the VMWare Interactive Shell (from SSH). decodeSel /var/log/ipmi_sel.raw Read IPMI system log of physical server. Replace vmid with the value you retrieved from the getallvms command (first one in the table).Replace path with the full path and file name of a VMX-file (= configuration file of a VM).","title":"VMware"},{"location":"platforms/vmware/#most-common-cli-commands-for-vmware-esxi","text":"Use SSH or esxi shells. vim-cmd vmsvc/getallvms List all VMs running on the host. Also provides vmid, required for commands below. vim-cmd vmsvc/power.off vmid Power off specified VM. vim-cmd vmsvc/power.on vmid Power off specified VM. vim-cmd vmsvc/power.reboot vmid Reboot specified VM. vim-cmd solo/registervm /vmfs/volume/datastore/subdir/vm-file.vmx Register the VM stored at location on the ESX host inventory. vim-cmd vmsvc/unregister vmid Unregister VM from the host. Does not remove the VM's files from the datastore. vim-cmd vmsvc/destroy vmid Delete the specified VM. The VMDK and VMX files will be deleted from storage as well. vim-cmd vmsvc/tools.install vmid Initiates an installation of VMWare Tools on the VM vim-cmd hostsvc/maintenance_mode_enter Put the host into maintenance mode. vim-cmd hostsvc/maintenance_mode_exit Take the host out of maintenance mode. vim-cmd hostsvc/net/info Show networking information of the host. chkconfig -l Show services running on the host. Can also be used to change startup configuration. esxtop Display list of processes and its usage of resources. Works similar to linux top. esxcfg-info Show host's configuration and information. esxcfg-nics -l Show current NIC configuration. esxcfg-vswitch -l Show current vSwitch configuration. vmkerrcode -l Display a reference list of VMKernel return codes and descriptions. dcui Start the console UI (when accessing through SSH). vsish Run the VMWare Interactive Shell (from SSH). decodeSel /var/log/ipmi_sel.raw Read IPMI system log of physical server. Replace vmid with the value you retrieved from the getallvms command (first one in the table).Replace path with the full path and file name of a VMX-file (= configuration file of a VM).","title":"Most common CLI commands for VMWare ESXi."},{"location":"security/nessus/","text":"A quick guide to carrying out some Sysadmin tasks on Nessus Version 7.x which is still supported. Version 8.x is the latest but due to the issues we had with this version, we should stay on version 7 for the foreseeable future or until we need to upgrade. The Nessus modules are automatically upgraded but the Nessus engine(components) won't be due to comments mentioned above. Where are the logs? /opt/nessus/var/nessus/logs/nessusd.messages Stop rebuild plugins and start service nessusd stop /opt/nessus/sbin/nessusd -R service nessusd start Generate a bug report The only time you'll need to do this is when you have to engage with the Tenable support. /opt/nessus/sbin/nessuscli bug-report-generator Re-registering Nessus from CLI service nessusd stop /opt/nessus/sbin/nessuscli fix --reset # /opt/nessus/sbin/nessuscli fetch --register <activation code> /opt/nessus/sbin/nessusd -R # service nessusd start Get the Scanner Metrics from CLI /opt/nessus/sbin/nessuscli fix --list | grep metrics Folders for Backup/Revcovery /opt/nessus/var/nessus/global.db /opt/nessus/var/nessus/master.key /opt/nessus/var/nessus/policies.db /opt/nessus/var/nessus/users /opt/nessus/etc/nessus Nessus Tuning Disable \"thorough test\" in scan policy. This can consume more resource Reduce \"max host per scan\" in scanner settings but this is a try it and see approach. Nessus Scan Schedules Description Frequency Day of Week Time External - Site 2 Full Scan Weekly Saturday 02:30 External - PCI Scan, Site 2 networks Monthly Saturday 12:00","title":"Nessus"},{"location":"security/nessus/#a-quick-guide-to-carrying-out-some-sysadmin-tasks-on-nessus","text":"Version 7.x which is still supported. Version 8.x is the latest but due to the issues we had with this version, we should stay on version 7 for the foreseeable future or until we need to upgrade. The Nessus modules are automatically upgraded but the Nessus engine(components) won't be due to comments mentioned above.","title":"A quick guide to carrying out some Sysadmin tasks on Nessus"},{"location":"security/nessus/#where-are-the-logs","text":"/opt/nessus/var/nessus/logs/nessusd.messages","title":"Where are the logs?"},{"location":"security/nessus/#stop-rebuild-plugins-and-start","text":"service nessusd stop /opt/nessus/sbin/nessusd -R service nessusd start","title":"Stop rebuild plugins and start"},{"location":"security/nessus/#generate-a-bug-report","text":"The only time you'll need to do this is when you have to engage with the Tenable support. /opt/nessus/sbin/nessuscli bug-report-generator","title":"Generate a bug report"},{"location":"security/nessus/#re-registering-nessus-from-cli","text":"service nessusd stop /opt/nessus/sbin/nessuscli fix --reset # /opt/nessus/sbin/nessuscli fetch --register <activation code> /opt/nessus/sbin/nessusd -R # service nessusd start","title":"Re-registering Nessus from CLI"},{"location":"security/nessus/#get-the-scanner-metrics-from-cli","text":"/opt/nessus/sbin/nessuscli fix --list | grep metrics","title":"Get the Scanner Metrics from CLI"},{"location":"security/nessus/#folders-for-backuprevcovery","text":"/opt/nessus/var/nessus/global.db /opt/nessus/var/nessus/master.key /opt/nessus/var/nessus/policies.db /opt/nessus/var/nessus/users /opt/nessus/etc/nessus","title":"Folders for Backup/Revcovery"},{"location":"security/nessus/#nessus-tuning","text":"Disable \"thorough test\" in scan policy. This can consume more resource Reduce \"max host per scan\" in scanner settings but this is a try it and see approach.","title":"Nessus Tuning"},{"location":"security/nessus/#nessus-scan-schedules","text":"Description Frequency Day of Week Time External - Site 2 Full Scan Weekly Saturday 02:30 External - PCI Scan, Site 2 networks Monthly Saturday 12:00","title":"Nessus Scan Schedules"},{"location":"security/openssl/","text":"Openssl Some stuff I came across while using openssl How to get a common name from an SSL certificate openssl x509 -noout -subject -in <path to cert> It will work with .pem etc. Remove a host from an authorized Key ssh-keygen -R hostname Basic SSH Hardening Only allow root logins via local terminal echo \"tty1\" > /etc/securetty chmod 700 /root","title":"openssl"},{"location":"security/openssl/#openssl","text":"Some stuff I came across while using openssl","title":"Openssl"},{"location":"security/openssl/#how-to-get-a-common-name-from-an-ssl-certificate","text":"openssl x509 -noout -subject -in <path to cert> It will work with .pem etc.","title":"How to get a common name from an SSL certificate"},{"location":"security/openssl/#remove-a-host-from-an-authorized-key","text":"ssh-keygen -R hostname","title":"Remove a host from an authorized Key"},{"location":"security/openssl/#basic-ssh-hardening","text":"Only allow root logins via local terminal echo \"tty1\" > /etc/securetty chmod 700 /root","title":"Basic SSH Hardening"},{"location":"security/selinux/","text":"SElinux I hate SElinux with a passion. So many issues I've come across are because of selinux. In an attempt to learn more about this here is some useful commands I've come across to mitigate some of the pain. Set individual domains to permissive Here is an examlpe for the httpd domain. semange permissive -a httpd_t Check for unconfined processes running in unconfined domains ps -eZ | grep unconfined_service_t","title":"SElinux"},{"location":"security/selinux/#selinux","text":"I hate SElinux with a passion. So many issues I've come across are because of selinux. In an attempt to learn more about this here is some useful commands I've come across to mitigate some of the pain.","title":"SElinux"},{"location":"security/selinux/#set-individual-domains-to-permissive","text":"Here is an examlpe for the httpd domain. semange permissive -a httpd_t","title":"Set individual domains to permissive"},{"location":"security/selinux/#check-for-unconfined-processes-running-in-unconfined-domains","text":"ps -eZ | grep unconfined_service_t","title":"Check for unconfined processes running in unconfined domains"},{"location":"windows/adduserstogroup/","text":"Useful one liner commands in Powershell Add a group of users from one AD group to another Get-ADGroupMember \u201c<Source AD Group>\u201d | Get-ADUser | ForEach-Object {Add-ADGroupMember -Identity \u201c<Destination AD Group>\u201d -Members $_}","title":"Powershell"},{"location":"windows/adduserstogroup/#useful-one-liner-commands-in-powershell","text":"","title":"Useful one liner commands in Powershell"},{"location":"windows/adduserstogroup/#add-a-group-of-users-from-one-ad-group-to-another","text":"Get-ADGroupMember \u201c<Source AD Group>\u201d | Get-ADUser | ForEach-Object {Add-ADGroupMember -Identity \u201c<Destination AD Group>\u201d -Members $_}","title":"Add a group of users from one AD group to another"},{"location":"windows/windows/","text":"RDP intermittantly freezes reg add \"HKLM\\software\\policies\\microsoft\\windows nt\\Terminal Services\\Client\" /v fClientDisableUDP /d 1 /t REG_DWORD","title":"RDP"},{"location":"windows/windows/#rdp-intermittantly-freezes","text":"reg add \"HKLM\\software\\policies\\microsoft\\windows nt\\Terminal Services\\Client\" /v fClientDisableUDP /d 1 /t REG_DWORD","title":"RDP intermittantly freezes"},{"location":"windows/wsl/","text":"WSL - Windows Linux Subsystem Some tips etc on how to use Windows WSL. Stop resolv.conf updating with local DNS If there is an issue connecting to some stuff defined in your WSL it is most likely DNS. To stop auto generation of resolv.conf you need to create a file /etc/wsl.conf and add the following, [network] generateResolvConf = false Then in a windows cmd line session, wsl --shutdown This shuts down wsl properly and saves any changes. How to import and export WSL distros Distros already installed and up to date from MS Store. Import a WSL distro Open cmd prompt as admin. wsl --list --all This will list all the WSL distros on your computer. To import, wsl --import <Name of distro> <path to save the <backupname>.tar filename Export a WSL distro Open cmd prompt as admin. wsl --list --all Pick your distro for export. wsl --export <Name of distro> <name of export>.tar file to export to To Uninstall imported WSL distros wsl --list --all Pick your distro wsl --unregister <Name of distro> And thats it. Fix issues with Github or SSH issues Modify the .bashrc file on the end with this, eval $(ssh-agent -s) && ssh-add ~/.ssh/<private key name> Obviously you can use an absolute path to point to wherever you store private keys you want to use when accessing remote hosts using ssh authentication with keys. DNS Fails on WSL after Upgrade to Version 2 Here is how to resolve the DNS issue when you up;grade WSL to version 2 and DNS stops working. I tried several different ones and this worked for me. In WSL rm /etc/resolv.conf || true rm /etc/wsl.conf || true Enable changing /etc/resolve.conf, Enable extended attributes on Windows drives cat <<EOF > /etc/wsl.conf [network] generateResolvConf = false [automount] enabled = true options = \"metadata\" mountFsTab = false EOF Use nameservers you define cat <<EOF > /etc/resolv.conf nameserver <your DNS to use> nameserver <your DNS to use> EOF Exit Linux WSL In Windows, cmd as admin wsl --shutdown netsh winsock reset netsh int ip reset all netsh winhttp reset proxy ipconfig /flushdns Reboot machine","title":"WSL"},{"location":"windows/wsl/#wsl-windows-linux-subsystem","text":"Some tips etc on how to use Windows WSL.","title":"WSL - Windows Linux Subsystem"},{"location":"windows/wsl/#stop-resolvconf-updating-with-local-dns","text":"If there is an issue connecting to some stuff defined in your WSL it is most likely DNS. To stop auto generation of resolv.conf you need to create a file /etc/wsl.conf and add the following, [network] generateResolvConf = false Then in a windows cmd line session, wsl --shutdown This shuts down wsl properly and saves any changes.","title":"Stop resolv.conf updating with local DNS"},{"location":"windows/wsl/#how-to-import-and-export-wsl-distros","text":"Distros already installed and up to date from MS Store.","title":"How to import and export WSL distros"},{"location":"windows/wsl/#import-a-wsl-distro","text":"Open cmd prompt as admin. wsl --list --all This will list all the WSL distros on your computer. To import, wsl --import <Name of distro> <path to save the <backupname>.tar filename","title":"Import a WSL distro"},{"location":"windows/wsl/#export-a-wsl-distro","text":"Open cmd prompt as admin. wsl --list --all Pick your distro for export. wsl --export <Name of distro> <name of export>.tar file to export to","title":"Export a WSL distro"},{"location":"windows/wsl/#to-uninstall-imported-wsl-distros","text":"wsl --list --all Pick your distro wsl --unregister <Name of distro> And thats it.","title":"To Uninstall imported WSL distros"},{"location":"windows/wsl/#fix-issues-with-github-or-ssh-issues","text":"Modify the .bashrc file on the end with this, eval $(ssh-agent -s) && ssh-add ~/.ssh/<private key name> Obviously you can use an absolute path to point to wherever you store private keys you want to use when accessing remote hosts using ssh authentication with keys.","title":"Fix issues with Github or SSH issues"},{"location":"windows/wsl/#dns-fails-on-wsl-after-upgrade-to-version-2","text":"Here is how to resolve the DNS issue when you up;grade WSL to version 2 and DNS stops working. I tried several different ones and this worked for me. In WSL rm /etc/resolv.conf || true rm /etc/wsl.conf || true Enable changing /etc/resolve.conf, Enable extended attributes on Windows drives cat <<EOF > /etc/wsl.conf [network] generateResolvConf = false [automount] enabled = true options = \"metadata\" mountFsTab = false EOF Use nameservers you define cat <<EOF > /etc/resolv.conf nameserver <your DNS to use> nameserver <your DNS to use> EOF Exit Linux WSL In Windows, cmd as admin wsl --shutdown netsh winsock reset netsh int ip reset all netsh winhttp reset proxy ipconfig /flushdns Reboot machine","title":"DNS Fails on WSL after Upgrade to Version 2"}]}